{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbxgOFvSPb6j"
   },
   "source": [
    "# Wine Price Prediction on Kaggle Dataset\n",
    "\n",
    "The aim is to predict wine prices from mixed inputs that contain both textual and categorical data. For this regression problem several neural based model were trained and their results have been discussed. The models make use of pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors of 100 dimentions. Hence, the glove.6B.100d needs to be downloaded in order to run the training and evaluation parts. Numpy, pandas, scikit learn and nltk was used for data manupulation and pre processing. Keras was used to build and train the machine learning models. \n",
    "\n",
    "Several different approaches were tried for this problem.\n",
    "\n",
    "## Model 1 (One Model)\n",
    "\n",
    "The first model makes use of pretrained word embeddings to come up with a representation of the wine descriptions in the datasets. For each description an average vector representation is constructed by averaging all the word vectors available in the description. Hence, a 100 dimention feature representation of the description column was obtained with such an approach. The intuition is that the word order might not be important since this is not a sequence to sequence problem and using an averaged representation would be sufficient for the description semantics. Additionally, the tf-idf values of each word in the descriptions were found so that the text can be vectorized in terms of tf-idf values. These, tf-idf vector representations and also average embedding vector representations were combined with several other categorical features that are selected from the dataset directly and then given to a neural network to learn the underlying regression function.  \n",
    "\n",
    "## Model 2 (Multi Model)\n",
    "\n",
    "The second model differs from the first model in the sense that the features are not given to the neural network by directly concatanating them. There are three different models making use of different features and these models are being concatanated in the end to make the final wine price prediction. For instance, vector average representations, tf-idf vector representation and categorical features obtained from the dataset are seperately fed into three different neural networks. Then, each of the outputs of these neural network contribute to the prediction of the wine price seperately. \n",
    "\n",
    "## Model 3 (LSTM Multi Model)\n",
    "\n",
    "The last model is almost identical to the second model. The only part where it differs is that the vector average representation is replaced with an LSTM that learns the representation of the description within the model using pretrained Glove embeddings. Again description representation, tf-idf feature and categorical features contribute to the prediction of wine price seperately. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Load Pretrained Glove vectors\n",
    "\n",
    "Please give the correct path for Glove embeddings and wine dataset csv.\n",
    "Additionally, I have used Google Colabs to train the models. Hence, I commented those lines of codes in case it is executed in a different environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39984,
     "status": "ok",
     "timestamp": 1557082096961,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "GafnkknpL9wP",
    "outputId": "9ce1c5fe-52ab-47ae-a608-9f78cf08b222"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras, nltk, string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#from google.colab import drive\n",
    "#from google.colab import files\n",
    "\n",
    "# This will prompt for authorization. \n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Setup NLTK packages if not present\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "   \n",
    "  \n",
    "# Helper funtion for loading Glove embeddings\n",
    "def load_glove(glove_path):\n",
    "    embeddings_index = {}\n",
    "    f = open(glove_path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "# Params\n",
    "GLOVE_PATH = \"glove.6B.100d.txt\"\n",
    "DATA_PATH = \"winemag-data-130k-v2.csv\"\n",
    "EMBEDDING_FEATS_PATH = \"embedding_feats.csv\"\n",
    "GLOVE_EMBED_SIZE = 100 \n",
    "LOAD_EMBEDDING_FEATS = False\n",
    "\n",
    "# Load glove embeddings\n",
    "embeddings=load_glove(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQebphVPavKu"
   },
   "source": [
    "## Load the data to Pandas and select the features to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43250,
     "status": "ok",
     "timestamp": 1557082100253,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "txcv93HyvGmC",
    "outputId": "fe6f75c0-c687-4a26-ee2f-0d84386b3d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129971 entries, 0 to 129970\n",
      "Data columns (total 14 columns):\n",
      "Unnamed: 0               129971 non-null int64\n",
      "country                  129908 non-null object\n",
      "description              129971 non-null object\n",
      "designation              92506 non-null object\n",
      "points                   129971 non-null int64\n",
      "price                    120975 non-null float64\n",
      "province                 129908 non-null object\n",
      "region_1                 108724 non-null object\n",
      "region_2                 50511 non-null object\n",
      "taster_name              103727 non-null object\n",
      "taster_twitter_handle    98758 non-null object\n",
      "title                    129971 non-null object\n",
      "variety                  129970 non-null object\n",
      "winery                   129971 non-null object\n",
      "dtypes: float64(1), int64(2), object(11)\n",
      "memory usage: 13.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load data to pandas data frame\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Show the general information regarding the dataset\n",
    "data.info()\n",
    "\n",
    "# Only select the columns that might be needed for the regression task\n",
    "data=data[['country','description','points','price','province','region_1','variety']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4aYbE6Na8xW"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First the data is checked column-wise to see the missing value percantages per column. None of the selected columns are exceeding the threshold of %50 missing instances, hence no need to eliminate any columns.\n",
    "\n",
    "The second step is to reduce the dimentionality of the categorical input representations by eliminating the instances that have frequencies below a certain threshold. The threshold were selected empirically by trial and error that aimed to keep the most of the training data but also reduce the dimentionality as much as possible so that the model can learn the problem easier.\n",
    "\n",
    "After the eliminations 22933 instance, we are left with a total of 107038 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60852,
     "status": "ok",
     "timestamp": 1557082117874,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "NvqOaqovQg0U",
    "outputId": "a96c95bc-bac5-4575-e28e-8fb7a4f50b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data size:  129971\n",
      "country        0.000485\n",
      "description    0.000000\n",
      "points         0.000000\n",
      "price          0.069215\n",
      "province       0.000485\n",
      "region_1       0.163475\n",
      "variety        0.000008\n",
      "dtype: float64\n",
      "Resulting data size:  107038 \n",
      "Number of eliminated instances:  22933\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>Riesling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>This dry and restrained wine offers spice in p...</td>\n",
       "      <td>87</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Gewürztraminer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Savory dried thyme notes accent sunnier flavor...</td>\n",
       "      <td>87</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Rheinhessen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gewürztraminer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>This has great depth of flavor with its fresh ...</td>\n",
       "      <td>87</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US</td>\n",
       "      <td>Soft, supple plum envelopes an oaky structure ...</td>\n",
       "      <td>87</td>\n",
       "      <td>19.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>This is a dry wine, very spicy, with a tight, ...</td>\n",
       "      <td>87</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Gewürztraminer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>Slightly reduced, this wine offers a chalky, t...</td>\n",
       "      <td>87</td>\n",
       "      <td>34.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Alexander Valley</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>US</td>\n",
       "      <td>Building on 150 years and six generations of w...</td>\n",
       "      <td>87</td>\n",
       "      <td>12.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Chardonnay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country                                        description  points  price  \\\n",
       "0       US  Tart and snappy, the flavors of lime flesh and...      87   14.0   \n",
       "1       US  Pineapple rind, lemon pith and orange blossom ...      87   13.0   \n",
       "2       US  Much like the regular bottling from 2012, this...      87   65.0   \n",
       "3   France  This dry and restrained wine offers spice in p...      87   24.0   \n",
       "4  Germany  Savory dried thyme notes accent sunnier flavor...      87   12.0   \n",
       "5   France  This has great depth of flavor with its fresh ...      87   27.0   \n",
       "6       US  Soft, supple plum envelopes an oaky structure ...      87   19.0   \n",
       "7   France  This is a dry wine, very spicy, with a tight, ...      87   30.0   \n",
       "8       US  Slightly reduced, this wine offers a chalky, t...      87   34.0   \n",
       "9       US  Building on 150 years and six generations of w...      87   12.0   \n",
       "\n",
       "      province             region_1             variety  \n",
       "0       Oregon    Willamette Valley          Pinot Gris  \n",
       "1     Michigan  Lake Michigan Shore            Riesling  \n",
       "2       Oregon    Willamette Valley          Pinot Noir  \n",
       "3       Alsace               Alsace      Gewürztraminer  \n",
       "4  Rheinhessen                  NaN      Gewürztraminer  \n",
       "5       Alsace               Alsace          Pinot Gris  \n",
       "6   California          Napa Valley  Cabernet Sauvignon  \n",
       "7       Alsace               Alsace      Gewürztraminer  \n",
       "8   California     Alexander Valley  Cabernet Sauvignon  \n",
       "9   California        Central Coast          Chardonnay  "
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_size=len(data)\n",
    "print(\"Initial data size: \",initial_size)\n",
    "\n",
    "# Print the percentage of NaNs that each column has in training data for analysis\n",
    "print(data.isnull().sum(axis=0)/len(data))\n",
    "\n",
    "# Define a threshold for dropping the columns with NaN percent above a certain threshold\n",
    "threshold=0.5\n",
    "\n",
    "# Drop the columns exceeding the threshold\n",
    "columns_drop=data.columns[data.isnull().sum(axis=0)/len(data)>threshold]\n",
    "data=data.drop(columns_drop,axis=1)\n",
    "\n",
    "# Do some preprocessing to eliminate rows that have frequency below some threshold\n",
    "# This operation is only applied to variety, region and province to reduce the feature input size\n",
    "# The threshold values are selected after empiracally trying several numbers\n",
    "variety_threshold = 100 \n",
    "#region_threshold = 0 \n",
    "province_threshold = 10 \n",
    "\n",
    "# Set the values below certain threshold as NaN\n",
    "variety_counts = data['variety'].value_counts()\n",
    "remove_variety = variety_counts[variety_counts <= variety_threshold].index\n",
    "data.replace(remove_variety, np.nan, inplace=True)\n",
    "\n",
    "#region_counts = data['region_1'].value_counts()\n",
    "#remove_region = region_counts[region_counts <= region_threshold].index\n",
    "#data.replace(remove_region, np.nan, inplace=True)\n",
    "\n",
    "province_counts = data['province'].value_counts()\n",
    "remove_province = province_counts[province_counts <= province_threshold].index\n",
    "data.replace(remove_province, np.nan, inplace=True)\n",
    "\n",
    "# Eliminate the rows with NaN values and reset the dataframe index for further use\n",
    "data.dropna(subset=['variety','country','price','province'],inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "print(\"Resulting data size: \",len(data), \"\\nNumber of eliminated instances: \",initial_size-len(data))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6Wtl8AhcbMD"
   },
   "source": [
    "## Average Vector Representation of Descriptions\n",
    "\n",
    "As explained earlier, Glove word embedding are utilized to come up with a  100 dimentional representation for each description in the dataset. Hence, these embeddings are averaged and the obtained vectors are saved as a seperate dataframe to be fed into the neural networks as features later on. Each  description is tokenized and then cleaned by eliminating the stopwords and lowercasing the tokens. Then, the lookups are made to the Glove embeddings dictionary. \n",
    "\n",
    "### This process can take slightly long amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1898
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2296477,
     "status": "ok",
     "timestamp": 1557084353515,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "MdSd2lY_XPC_",
    "outputId": "6f221151-8385-4193-b3f0-5dbf099288a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107038\n",
      "Average Glove vector progress:  0.9342476503671593\n",
      "Average Glove vector progress:  1.8684953007343186\n",
      "Average Glove vector progress:  2.802742951101478\n",
      "Average Glove vector progress:  3.736990601468637\n",
      "Average Glove vector progress:  4.671238251835796\n",
      "Average Glove vector progress:  5.605485902202956\n",
      "Average Glove vector progress:  6.539733552570115\n",
      "Average Glove vector progress:  7.473981202937274\n",
      "Average Glove vector progress:  8.408228853304434\n",
      "Average Glove vector progress:  9.342476503671593\n",
      "Average Glove vector progress:  10.276724154038753\n",
      "Average Glove vector progress:  11.210971804405911\n",
      "Average Glove vector progress:  12.145219454773072\n",
      "Average Glove vector progress:  13.07946710514023\n",
      "Average Glove vector progress:  14.01371475550739\n",
      "Average Glove vector progress:  14.947962405874549\n",
      "Average Glove vector progress:  15.882210056241709\n",
      "Average Glove vector progress:  16.81645770660887\n",
      "Average Glove vector progress:  17.750705356976027\n",
      "Average Glove vector progress:  18.684953007343186\n",
      "Average Glove vector progress:  19.619200657710344\n",
      "Average Glove vector progress:  20.553448308077506\n",
      "Average Glove vector progress:  21.487695958444665\n",
      "Average Glove vector progress:  22.421943608811823\n",
      "Average Glove vector progress:  23.356191259178985\n",
      "Average Glove vector progress:  24.290438909546143\n",
      "Average Glove vector progress:  25.2246865599133\n",
      "Average Glove vector progress:  26.15893421028046\n",
      "Average Glove vector progress:  27.093181860647622\n",
      "Average Glove vector progress:  28.02742951101478\n",
      "Average Glove vector progress:  28.96167716138194\n",
      "Average Glove vector progress:  29.895924811749097\n",
      "Average Glove vector progress:  30.83017246211626\n",
      "Average Glove vector progress:  31.764420112483418\n",
      "Average Glove vector progress:  32.698667762850576\n",
      "Average Glove vector progress:  33.63291541321774\n",
      "Average Glove vector progress:  34.56716306358489\n",
      "Average Glove vector progress:  35.501410713952055\n",
      "Average Glove vector progress:  36.43565836431922\n",
      "Average Glove vector progress:  37.36990601468637\n",
      "Average Glove vector progress:  38.30415366505353\n",
      "Average Glove vector progress:  39.23840131542069\n",
      "Average Glove vector progress:  40.17264896578785\n",
      "Average Glove vector progress:  41.10689661615501\n",
      "Average Glove vector progress:  42.04114426652217\n",
      "Average Glove vector progress:  42.97539191688933\n",
      "Average Glove vector progress:  43.90963956725649\n",
      "Average Glove vector progress:  44.843887217623646\n",
      "Average Glove vector progress:  45.77813486799081\n",
      "Average Glove vector progress:  46.71238251835797\n",
      "Average Glove vector progress:  47.646630168725125\n",
      "Average Glove vector progress:  48.58087781909229\n",
      "Average Glove vector progress:  49.51512546945944\n",
      "Average Glove vector progress:  50.4493731198266\n",
      "Average Glove vector progress:  51.383620770193765\n",
      "Average Glove vector progress:  52.31786842056092\n",
      "Average Glove vector progress:  53.25211607092808\n",
      "Average Glove vector progress:  54.186363721295244\n",
      "Average Glove vector progress:  55.1206113716624\n",
      "Average Glove vector progress:  56.05485902202956\n",
      "Average Glove vector progress:  56.989106672396716\n",
      "Average Glove vector progress:  57.92335432276388\n",
      "Average Glove vector progress:  58.85760197313104\n",
      "Average Glove vector progress:  59.791849623498194\n",
      "Average Glove vector progress:  60.726097273865356\n",
      "Average Glove vector progress:  61.66034492423252\n",
      "Average Glove vector progress:  62.59459257459967\n",
      "Average Glove vector progress:  63.528840224966835\n",
      "Average Glove vector progress:  64.463087875334\n",
      "Average Glove vector progress:  65.39733552570115\n",
      "Average Glove vector progress:  66.3315831760683\n",
      "Average Glove vector progress:  67.26583082643548\n",
      "Average Glove vector progress:  68.20007847680263\n",
      "Average Glove vector progress:  69.13432612716979\n",
      "Average Glove vector progress:  70.06857377753695\n",
      "Average Glove vector progress:  71.00282142790411\n",
      "Average Glove vector progress:  71.93706907827126\n",
      "Average Glove vector progress:  72.87131672863843\n",
      "Average Glove vector progress:  73.80556437900559\n",
      "Average Glove vector progress:  74.73981202937274\n",
      "Average Glove vector progress:  75.67405967973991\n",
      "Average Glove vector progress:  76.60830733010707\n",
      "Average Glove vector progress:  77.54255498047422\n",
      "Average Glove vector progress:  78.47680263084138\n",
      "Average Glove vector progress:  79.41105028120855\n",
      "Average Glove vector progress:  80.3452979315757\n",
      "Average Glove vector progress:  81.27954558194286\n",
      "Average Glove vector progress:  82.21379323231002\n",
      "Average Glove vector progress:  83.14804088267718\n",
      "Average Glove vector progress:  84.08228853304433\n",
      "Average Glove vector progress:  85.0165361834115\n",
      "Average Glove vector progress:  85.95078383377866\n",
      "Average Glove vector progress:  86.88503148414581\n",
      "Average Glove vector progress:  87.81927913451298\n",
      "Average Glove vector progress:  88.75352678488014\n",
      "Average Glove vector progress:  89.68777443524729\n",
      "Average Glove vector progress:  90.62202208561446\n",
      "Average Glove vector progress:  91.55626973598162\n",
      "Average Glove vector progress:  92.49051738634877\n",
      "Average Glove vector progress:  93.42476503671594\n",
      "Average Glove vector progress:  94.3590126870831\n",
      "Average Glove vector progress:  95.29326033745025\n",
      "Average Glove vector progress:  96.2275079878174\n",
      "Average Glove vector progress:  97.16175563818457\n",
      "Average Glove vector progress:  98.09600328855173\n",
      "Average Glove vector progress:  99.03025093891888\n",
      "Average Glove vector progress:  99.96449858928605\n",
      "This dry and restrained wine offers spice in profusion. Balanced with acidity and a firm texture, it's very much for food.\n",
      "This is a dry wine, very spicy, with a tight, taut texture and strongly mineral character layered with citrus as well as pepper. It's a food wine with its almost crisp aftertaste.\n",
      "Cosine similarity between descriptions:  0.9416637871378292\n",
      "Embedding features shape:  (107038, 100)\n"
     ]
    }
   ],
   "source": [
    "# Define cosine similarity for measuring the similarity between average Glove vector representation of descriptions\n",
    "def cosine_sim(a,b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    cos = dot / (norma * normb)\n",
    "    return cos\n",
    "\n",
    "# Choose whether to load a previously extracted average embedding feature file or extract them now\n",
    "if LOAD_EMBEDDING_FEATS:\n",
    "  print(\"Loading embedding features from file: \", EMBEDDING_FEATS_PATH)\n",
    "  embedding_feats = pd.read_csv(EMBEDDING_FEATS_PATH)\n",
    "  embedding_feats.drop(embedding_feats.columns[0],axis=1,inplace=True)\n",
    "else:\n",
    "  # Select the descriptions from dataframe and create an initial numpy array that will hold average vector per description\n",
    "  descriptions=data['description']\n",
    "  desc_avg_vecs=np.zeros((1,100))\n",
    "\n",
    "  total_processed=0\n",
    "  print(len(descriptions))\n",
    "  for description in descriptions:\n",
    "      # Clean the description by eliminating stop words, punctuations and lowercasing the text\n",
    "      desc_clean = [i for i in nltk.word_tokenize(description.lower()) if i not in list(string.punctuation)+nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "      # Define a numpy array for summing up each word vector within description\n",
    "      sum_vec=np.zeros(100)\n",
    "      count=0\n",
    "      for word in desc_clean:\n",
    "          if word in embeddings:\n",
    "              sum_vec=np.add(sum_vec,embeddings[word])\n",
    "              count+=1\n",
    "\n",
    "      # Add the averaged vector of the current description    \n",
    "      desc_avg_vecs=np.vstack((desc_avg_vecs,sum_vec/count))\n",
    "\n",
    "      # For tracking the progress\n",
    "      total_processed+=1\n",
    "      if(total_processed%1000==0):\n",
    "          print(\"Average Glove vector progress: \",100*total_processed/len(descriptions))\n",
    "\n",
    "  desc_avg_vecs=desc_avg_vecs[1:]\n",
    "\n",
    "  # Cosine similarity of two descriptions\n",
    "  print(data['description'].iloc[3])\n",
    "  print(data['description'].iloc[7])\n",
    "  print(\"Cosine similarity between descriptions: \", cosine_sim(desc_avg_vecs[3],desc_avg_vecs[7]))\n",
    "\n",
    "  # Load the description average vector features to a dataframe for future use\n",
    "  embedding_feats=pd.DataFrame(desc_avg_vecs)\n",
    "\n",
    "  # Save the embedding features to csv\n",
    "  embedding_feats.to_csv(EMBEDDING_FEATS_PATH)\n",
    "\n",
    "# Resulting feature size of descriptions\n",
    "print(\"Embedding features shape: \", embedding_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcbpBBKZdzil"
   },
   "source": [
    "## Extract TF-IDF Features and convert Categorical Attributes to One-Hot Vectors\n",
    "\n",
    "For categorical attributes, country, province, variety and points were used since the other columns (taster name, twitter, handle) seemed irrelvant or had a lot of missing values (region_2, region_3) or increased the input dimentionality which is not desired when predicting the wine prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2299850,
     "status": "ok",
     "timestamp": 1557084356892,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "Td5eE_lErIcz",
    "outputId": "86d2088d-3de0-4f05-c834-1f9a0252d379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings features shape:  (107038, 100) \n",
      "tf-idf feature shape:  (107038, 10000) \n",
      "variety feature shape:  (107038, 88) \n",
      "province feature shape:  (107038, 166) \n",
      "country feature shape:  (107038, 24) \n",
      "points feature shape:  (107038, 21)\n"
     ]
    }
   ],
   "source": [
    "# Extract tf-idf features from the descriptions\n",
    "vectorizer = TfidfVectorizer(stop_words='english',ngram_range =(1,1),max_features=10000)\n",
    "tf_idf_feats = vectorizer.fit_transform(data['description'])\n",
    "\n",
    "# Use sklearn utility to convert label strings to numbered index\n",
    "columns_to_encode=['country','province','variety','points']\n",
    "le = LabelEncoder()\n",
    "data.loc[:,columns_to_encode]=data.loc[:, columns_to_encode].apply(le.fit_transform)\n",
    "\n",
    "country_num_classes = np.max(data['country']) + 1\n",
    "province_num_classes = np.max(data['province']) + 1\n",
    "variety_num_classes = np.max(data['variety']) + 1\n",
    "points_num_classes = np.max(data['points']) + 1\n",
    "\n",
    "# Convert labels to one hot\n",
    "country_feats = keras.utils.to_categorical(data['country'], country_num_classes)\n",
    "province_feats = keras.utils.to_categorical(data['province'], province_num_classes)\n",
    "variety_feats = keras.utils.to_categorical(data['variety'], variety_num_classes)\n",
    "points_feats = keras.utils.to_categorical(data['points'], points_num_classes)\n",
    "\n",
    "print(\"Embeddings features shape: \", embedding_feats.shape,\n",
    "      \"\\ntf-idf feature shape: \", tf_idf_feats.shape,\n",
    "      \"\\nvariety feature shape: \", variety_feats.shape,\n",
    "      \"\\nprovince feature shape: \", province_feats.shape,\n",
    "      \"\\ncountry feature shape: \", country_feats.shape, \n",
    "      \"\\npoints feature shape: \", points_feats.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26usN8aieEFj"
   },
   "source": [
    "## Split the data into Train and Test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2299850,
     "status": "ok",
     "timestamp": 1557084356894,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "v6dKxZ_JsboO",
    "outputId": "dacb124c-12e2-44e6-fb35-4763fd8c3bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 85630\n",
      "Test size: 21408\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "train_size = int(len(data) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))\n",
    "\n",
    "# Train features\n",
    "embedding_feats_train = embedding_feats[:train_size]\n",
    "tf_idf_feats_train = tf_idf_feats[:train_size]\n",
    "variety_feats_train = variety_feats[:train_size]\n",
    "province_feats_train = province_feats[:train_size]\n",
    "country_feats_train = country_feats[:train_size]\n",
    "points_feats_train = points_feats[:train_size]\n",
    "\n",
    "# Train labels\n",
    "labels_train = data['price'][:train_size]\n",
    "\n",
    "# Test features\n",
    "embedding_feats_test = embedding_feats[train_size:]\n",
    "tf_idf_feats_test = tf_idf_feats[train_size:]\n",
    "variety_feats_test = variety_feats[train_size:]\n",
    "province_feats_test = province_feats[train_size:]\n",
    "country_feats_test = country_feats[train_size:]\n",
    "points_feats_test = points_feats[train_size:]\n",
    "\n",
    "# Test labels\n",
    "labels_test = data['price'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZT7XIcSVeWUd"
   },
   "source": [
    "## Prepare the input data for the LSTM part of Model 3\n",
    "\n",
    "The description text is preprocessed by making use of Keras tokenizer. Then the tokenized text is converted into sequences. Importantly padding is applied with respect to the longest sequence in the data. Lastly, an embedding matrix that only contains the words ids and their Glove vector representations is created to initialize the Embedding layer of the LSTM of Model 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gLpLv7H1Y8m"
   },
   "outputs": [],
   "source": [
    "# Method required for making use of pretrained embeddings in LSTM embedding layer\n",
    "def prepare_embedding_weights(word2id,embedding_size, glove_embeddings):\n",
    "  embedding_matrix = np.zeros((len(word2id) + 1, embedding_size))\n",
    "  for word, i in word2id.items():\n",
    "      embedding_vector = glove_embeddings.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "\n",
    "  return embedding_matrix\n",
    "\n",
    "\n",
    "# Create a Keras tokenizer to preprocess descriptions\n",
    "desc_train = data['description'][:train_size]\n",
    "desc_test = data['description'][train_size:]\n",
    "\n",
    "# Limit the max vocab size\n",
    "MAX_VOCAB_SIZE = 15000\n",
    "\n",
    "# Tokenize and fit on training text\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_VOCAB_SIZE, char_level=False)\n",
    "tokenizer.fit_on_texts(data['description'][:train_size])\n",
    "\n",
    "# Convert tokenized text to padded sequences for LSTM\n",
    "train_seqs = tokenizer.texts_to_sequences(desc_train)\n",
    "test_seqs = tokenizer.texts_to_sequences(desc_test)\n",
    "\n",
    "# Find the max sequence length in the corpus in order to pad with correct length\n",
    "MAX_SEQ_LEN = max(len(max(train_seqs, key=len)),len(max(test_seqs, key=len)))\n",
    "train_seqs_padded = keras.preprocessing.sequence.pad_sequences(train_seqs, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "test_seqs_padded = keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "# Form the embedding layers weight matrix from pretrained Glove vectors\n",
    "embedding_matrix = prepare_embedding_weights(tokenizer.word_index, GLOVE_EMBED_SIZE, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjVpz5YyVAhB"
   },
   "source": [
    "## Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsjtP1RMxozv"
   },
   "outputs": [],
   "source": [
    "# Model 1\n",
    "def create_one_model():\n",
    "  # Input layer\n",
    "  embed_inputs = keras.layers.Input(shape=(embedding_feats.shape[1],))\n",
    "  tf_idf_inputs = keras.layers.Input(shape=(tf_idf_feats.shape[1],))\n",
    "  variety_inputs = keras.layers.Input(shape=(variety_feats.shape[1],))\n",
    "  province_inputs = keras.layers.Input(shape=(province_feats.shape[1],))\n",
    "  country_inputs = keras.layers.Input(shape=(country_feats.shape[1],))\n",
    "  points_inputs = keras.layers.Input(shape=(points_feats.shape[1],))\n",
    "\n",
    "  # Hidden Layer\n",
    "  concat_input_layer = keras.layers.concatenate([embed_inputs, tf_idf_inputs, variety_inputs,province_inputs, country_inputs, points_inputs])\n",
    "  concat_input_layer = keras.layers.Dense(32, activation='relu',kernel_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01))(concat_input_layer)\n",
    "\n",
    "  #Output layer\n",
    "  output = keras.layers.Dense(1,activation='linear')(concat_input_layer)\n",
    "  model = keras.Model(inputs=[embed_inputs,tf_idf_inputs, variety_inputs,province_inputs, country_inputs, points_inputs], outputs=output)\n",
    "  model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "  print(model.summary())\n",
    "  \n",
    "  return model\n",
    "\n",
    "# Model 2\n",
    "def create_multi_model():\n",
    "  # Input layer\n",
    "  embed_inputs = keras.layers.Input(shape=(embedding_feats.shape[1],))\n",
    "  tf_idf_inputs = keras.layers.Input(shape=(tf_idf_feats.shape[1],))\n",
    "  variety_inputs = keras.layers.Input(shape=(variety_feats.shape[1],))\n",
    "  province_inputs = keras.layers.Input(shape=(province_feats.shape[1],))\n",
    "  country_inputs = keras.layers.Input(shape=(country_feats.shape[1],))\n",
    "  points_inputs = keras.layers.Input(shape=(points_feats.shape[1],))\n",
    "  \n",
    "  # Embedding model\n",
    "  embed_layer_1 = keras.layers.Dense(32, activation=\"relu\",kernel_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01))(embed_inputs)\n",
    "  embed_layer_2 = keras.layers.Dense(1, activation=\"linear\")(embed_layer_1)\n",
    "  embed_model = keras.Model(inputs=embed_inputs, outputs=embed_layer_2)\n",
    "\n",
    "  # tf-idf model\n",
    "  tf_idf_layer_1 = keras.layers.Dense(32, activation=\"relu\",kernel_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01))(tf_idf_inputs)\n",
    "  tf_idf_layer_2 = keras.layers.Dense(1, activation=\"linear\")(tf_idf_layer_1)\n",
    "  tf_idf_model = keras.Model(inputs=tf_idf_inputs, outputs=tf_idf_layer_2)\n",
    "\n",
    "  # Categorical features model\n",
    "  categorical_input_layer = keras.layers.concatenate([variety_inputs,province_inputs, country_inputs, points_inputs])\n",
    "  categorical_layer_1 = keras.layers.Dense(32, activation='relu',kernel_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01))(categorical_input_layer)\n",
    "  categorical_layer_2 = keras.layers.Dense(1, activation='linear')(categorical_layer_1)\n",
    "  categorical_model = keras.Model(inputs=[variety_inputs,province_inputs, country_inputs, points_inputs], outputs=categorical_layer_2)\n",
    "\n",
    "  combined = keras.layers.concatenate([embed_model.output, tf_idf_model.output, categorical_model.output])\n",
    "  output = keras.layers.Dense(1, activation=\"linear\")(combined)\n",
    "  model = keras.Model(inputs=[embed_inputs,tf_idf_inputs, variety_inputs,province_inputs, country_inputs, points_inputs], outputs=output)\n",
    "  model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "  print(model.summary())\n",
    "  \n",
    "  return model\n",
    "\n",
    "# Model 3\n",
    "def create_lstm_multi_model():\n",
    "  hidden_size=32\n",
    "  vocab_size=embedding_matrix.shape[0]\n",
    "  \n",
    "  # Input layer\n",
    "  tf_idf_inputs = keras.layers.Input(shape=(tf_idf_feats.shape[1],))\n",
    "  variety_inputs = keras.layers.Input(shape=(variety_feats.shape[1],))\n",
    "  province_inputs = keras.layers.Input(shape=(province_feats.shape[1],))\n",
    "  country_inputs = keras.layers.Input(shape=(country_feats.shape[1],))\n",
    "  points_inputs = keras.layers.Input(shape=(points_feats.shape[1],))\n",
    "  \n",
    "  # Description LSTM embeddings\n",
    "  desc_inputs = keras.layers.Input(shape=(MAX_SEQ_LEN,))\n",
    "  embedding_layer = keras.layers.Embedding(vocab_size, GLOVE_EMBED_SIZE, input_length=MAX_SEQ_LEN, weights=[embedding_matrix], trainable=False)(desc_inputs)\n",
    "  if(len(keras.backend.tensorflow_backend._get_available_gpus()) > 0):\n",
    "    lstm_layer = keras.layers.CuDNNLSTM(hidden_size)(embedding_layer)\n",
    "  else:\n",
    "    lstm_layer = keras.layers.LSTM(hidden_size)(embedding_layer)\n",
    "  lstm_output = keras.layers.Dense(1, activation=\"linear\")(lstm_layer)\n",
    "  lstm_model = keras.Model(inputs=desc_inputs, outputs=lstm_output)\n",
    "\n",
    "  # tf-idf model\n",
    "  tf_idf_layer_1 = keras.layers.Dense(32, activation=\"relu\",kernel_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01))(tf_idf_inputs)\n",
    "  tf_idf_output = keras.layers.Dense(1, activation=\"linear\")(tf_idf_layer_1)\n",
    "  tf_idf_model = keras.Model(inputs=tf_idf_inputs, outputs=tf_idf_output)\n",
    "\n",
    "  # Categorical features model\n",
    "  categorical_input_layer = keras.layers.concatenate([variety_inputs,province_inputs, country_inputs, points_inputs])\n",
    "  categorical_layer_1 = keras.layers.Dense(32, activation='relu',kernel_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01))(categorical_input_layer)\n",
    "  categorical_output = keras.layers.Dense(1, activation='linear')(categorical_layer_1)\n",
    "  categorical_model = keras.Model(inputs=[variety_inputs,province_inputs, country_inputs, points_inputs], outputs=categorical_output)\n",
    "\n",
    "  combined = keras.layers.concatenate([lstm_model.output, tf_idf_model.output, categorical_model.output])\n",
    "  output = keras.layers.Dense(1, activation=\"linear\")(combined)\n",
    "  model = keras.Model(inputs=[desc_inputs, tf_idf_inputs, variety_inputs,province_inputs, country_inputs, points_inputs], outputs=output)\n",
    "  model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "  print(model.summary())\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSJKKEVbkrrB"
   },
   "source": [
    "## Model Decisions\n",
    "\n",
    "In order to build a better model which does not overfit or underfit, I have tried several approaches towards hyperparameter optimization. \n",
    "\n",
    "\n",
    "*   Firstly, when I increased the model complexity by either adding more layers or increasing the hidden layer sizes to more than 128 it didn't help and the models started to overfit. Hence, I either decreased the hidden layer size or the number of layers in order to reduce the model complexity. \n",
    "\n",
    "*   Secondly, L1 & L2 regularization was utilized. L1 is important in this case because there were lots of categorical features with sparse vectors and it is known that L1 performs better when there are sparse input vectors. Combined with L2 regularization the models started to fit better. \n",
    "*  Thirdly, I also tweaked the batch size and saw that when I increased the batch size too much, the number of training examples were not enough for the gradient descent and the models again suffered in terms of quality. Hence batch size of 64 seemed to be optimal for this dataset. \n",
    "*  It is obvious but relu also helps with gradients more specifically for RNN in model 3\n",
    "*  Choosing the evalution metric as both MSE and MAE helped review the results much more efficiently and easier\n",
    "*  Importantly, early stopping criteria was also useful for not training the model further in the cases where the validation loss did not improve\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGde5BixQ_U1"
   },
   "outputs": [],
   "source": [
    "early_stopping_monitor = keras.callbacks.EarlyStopping(patience=3)\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "def plot_losses(history):\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('model loss')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'validation'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def train_and_evaluate(model_name):\n",
    "  if model_name == 1:\n",
    "    model = create_one_model()\n",
    "    history = model.fit([embedding_feats_train]+[tf_idf_feats_train]+[variety_feats_train]+[province_feats_train]+[country_feats_train]+[points_feats_train], labels_train, \n",
    "                        epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,  verbose=1, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "    predictions = model.predict([embedding_feats_test, tf_idf_feats_test, variety_feats_test, province_feats_test, country_feats_test, points_feats_test])\n",
    "\n",
    "  elif model_name == 2:\n",
    "    model = create_multi_model();\n",
    "    history = model.fit([embedding_feats_train]+[tf_idf_feats_train]+[variety_feats_train]+[province_feats_train]+[country_feats_train]+[points_feats_train], labels_train,\n",
    "                        epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,  verbose=1, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "    predictions = model.predict([embedding_feats_test, tf_idf_feats_test, variety_feats_test, province_feats_test, country_feats_test, points_feats_test])\n",
    "\n",
    "  else:\n",
    "    model = create_lstm_multi_model()\n",
    "    history = model.fit([train_seqs_padded]+[tf_idf_feats_train]+[variety_feats_train]+[province_feats_train]+[country_feats_train]+[points_feats_train], labels_train, \n",
    "                        epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,  verbose=1, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "    predictions = model.predict([test_seqs_padded, tf_idf_feats_test, variety_feats_test, province_feats_test, country_feats_test, points_feats_test])\n",
    "  \n",
    "  \n",
    "  return predictions, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvJhgCx3oj8A"
   },
   "source": [
    "## Train and evaluate all the models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1081348,
     "status": "ok",
     "timestamp": 1557087200895,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "OFmAqc_xles0",
    "outputId": "d120ceb3-2bd3-4e09-a39e-b1464fe4d3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           (None, 10000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_39 (InputLayer)           (None, 88)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           (None, 166)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_41 (InputLayer)           (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10399)        0           input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "                                                                 input_39[0][0]                   \n",
      "                                                                 input_40[0][0]                   \n",
      "                                                                 input_41[0][0]                   \n",
      "                                                                 input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           332800      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            33          dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 332,833\n",
      "Trainable params: 332,833\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 68504 samples, validate on 17126 samples\n",
      "Epoch 1/50\n",
      "68504/68504 [==============================] - 7s 107us/step - loss: 1708.5895 - mean_squared_error: 1690.0032 - mean_absolute_error: 18.9506 - val_loss: 1494.4019 - val_mean_squared_error: 1469.7967 - val_mean_absolute_error: 15.7620\n",
      "Epoch 2/50\n",
      "68504/68504 [==============================] - 7s 98us/step - loss: 1381.7590 - mean_squared_error: 1353.9710 - mean_absolute_error: 15.1128 - val_loss: 1367.0028 - val_mean_squared_error: 1336.8482 - val_mean_absolute_error: 14.9078\n",
      "Epoch 3/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1313.1262 - mean_squared_error: 1282.3409 - mean_absolute_error: 14.6416 - val_loss: 1312.7008 - val_mean_squared_error: 1281.5301 - val_mean_absolute_error: 14.2549\n",
      "Epoch 4/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1274.1205 - mean_squared_error: 1241.6111 - mean_absolute_error: 14.1978 - val_loss: 1276.3780 - val_mean_squared_error: 1242.7985 - val_mean_absolute_error: 13.8713\n",
      "Epoch 5/50\n",
      "68504/68504 [==============================] - 7s 98us/step - loss: 1245.8680 - mean_squared_error: 1211.3128 - mean_absolute_error: 14.0241 - val_loss: 1250.1956 - val_mean_squared_error: 1215.3406 - val_mean_absolute_error: 13.5829\n",
      "Epoch 6/50\n",
      "68504/68504 [==============================] - 7s 105us/step - loss: 1223.4879 - mean_squared_error: 1187.6675 - mean_absolute_error: 13.7698 - val_loss: 1226.1207 - val_mean_squared_error: 1189.3569 - val_mean_absolute_error: 13.7939\n",
      "Epoch 7/50\n",
      "68504/68504 [==============================] - 7s 104us/step - loss: 1202.5553 - mean_squared_error: 1164.7120 - mean_absolute_error: 13.5972 - val_loss: 1207.0479 - val_mean_squared_error: 1168.9046 - val_mean_absolute_error: 13.3964\n",
      "Epoch 8/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1183.9435 - mean_squared_error: 1144.9963 - mean_absolute_error: 13.4044 - val_loss: 1192.0153 - val_mean_squared_error: 1151.8986 - val_mean_absolute_error: 13.2559\n",
      "Epoch 9/50\n",
      "68504/68504 [==============================] - 7s 96us/step - loss: 1167.8080 - mean_squared_error: 1127.1594 - mean_absolute_error: 13.3170 - val_loss: 1179.2241 - val_mean_squared_error: 1137.6836 - val_mean_absolute_error: 13.3237\n",
      "Epoch 10/50\n",
      "68504/68504 [==============================] - 7s 98us/step - loss: 1154.1763 - mean_squared_error: 1111.8077 - mean_absolute_error: 13.2110 - val_loss: 1169.4066 - val_mean_squared_error: 1126.7349 - val_mean_absolute_error: 13.3779\n",
      "Epoch 11/50\n",
      "68504/68504 [==============================] - 7s 96us/step - loss: 1141.7196 - mean_squared_error: 1098.3492 - mean_absolute_error: 13.1139 - val_loss: 1159.4324 - val_mean_squared_error: 1115.2636 - val_mean_absolute_error: 13.1142\n",
      "Epoch 12/50\n",
      "68504/68504 [==============================] - 7s 96us/step - loss: 1129.8704 - mean_squared_error: 1085.0702 - mean_absolute_error: 13.0312 - val_loss: 1149.4269 - val_mean_squared_error: 1103.9699 - val_mean_absolute_error: 12.9336\n",
      "Epoch 13/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1119.0874 - mean_squared_error: 1072.9350 - mean_absolute_error: 12.9563 - val_loss: 1144.2317 - val_mean_squared_error: 1097.7475 - val_mean_absolute_error: 13.2346\n",
      "Epoch 14/50\n",
      "68504/68504 [==============================] - 7s 96us/step - loss: 1108.6998 - mean_squared_error: 1061.1455 - mean_absolute_error: 12.9101 - val_loss: 1134.9443 - val_mean_squared_error: 1087.1709 - val_mean_absolute_error: 12.9953\n",
      "Epoch 15/50\n",
      "68504/68504 [==============================] - 7s 95us/step - loss: 1099.5139 - mean_squared_error: 1050.8450 - mean_absolute_error: 12.8367 - val_loss: 1127.3237 - val_mean_squared_error: 1078.3879 - val_mean_absolute_error: 12.6425\n",
      "Epoch 16/50\n",
      "68504/68504 [==============================] - 7s 98us/step - loss: 1090.5066 - mean_squared_error: 1040.8088 - mean_absolute_error: 12.7765 - val_loss: 1123.1560 - val_mean_squared_error: 1072.7757 - val_mean_absolute_error: 12.6069\n",
      "Epoch 17/50\n",
      "68504/68504 [==============================] - 7s 98us/step - loss: 1081.8416 - mean_squared_error: 1030.7074 - mean_absolute_error: 12.7191 - val_loss: 1116.9274 - val_mean_squared_error: 1065.7266 - val_mean_absolute_error: 12.7353\n",
      "Epoch 18/50\n",
      "68504/68504 [==============================] - 7s 105us/step - loss: 1074.7211 - mean_squared_error: 1022.9757 - mean_absolute_error: 12.6509 - val_loss: 1111.1134 - val_mean_squared_error: 1058.6769 - val_mean_absolute_error: 12.5464\n",
      "Epoch 19/50\n",
      "68504/68504 [==============================] - 7s 105us/step - loss: 1066.4251 - mean_squared_error: 1013.2483 - mean_absolute_error: 12.6086 - val_loss: 1107.3503 - val_mean_squared_error: 1054.1217 - val_mean_absolute_error: 12.5872\n",
      "Epoch 20/50\n",
      "68504/68504 [==============================] - 7s 106us/step - loss: 1060.2921 - mean_squared_error: 1006.3459 - mean_absolute_error: 12.5724 - val_loss: 1104.5846 - val_mean_squared_error: 1049.8725 - val_mean_absolute_error: 12.3488\n",
      "Epoch 21/50\n",
      "68504/68504 [==============================] - 7s 108us/step - loss: 1052.9177 - mean_squared_error: 997.6629 - mean_absolute_error: 12.5408 - val_loss: 1101.9657 - val_mean_squared_error: 1046.5560 - val_mean_absolute_error: 12.5995\n",
      "Epoch 22/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1047.6841 - mean_squared_error: 991.4727 - mean_absolute_error: 12.4808 - val_loss: 1096.7015 - val_mean_squared_error: 1040.4219 - val_mean_absolute_error: 12.5324\n",
      "Epoch 23/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1041.3802 - mean_squared_error: 984.5098 - mean_absolute_error: 12.4565 - val_loss: 1095.2132 - val_mean_squared_error: 1038.0124 - val_mean_absolute_error: 12.1951\n",
      "Epoch 24/50\n",
      "68504/68504 [==============================] - 7s 96us/step - loss: 1036.2228 - mean_squared_error: 978.4877 - mean_absolute_error: 12.4160 - val_loss: 1091.8319 - val_mean_squared_error: 1033.6490 - val_mean_absolute_error: 12.4456\n",
      "Epoch 25/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1030.2201 - mean_squared_error: 971.1596 - mean_absolute_error: 12.3848 - val_loss: 1089.6907 - val_mean_squared_error: 1030.5526 - val_mean_absolute_error: 12.4681\n",
      "Epoch 26/50\n",
      "68504/68504 [==============================] - 7s 96us/step - loss: 1025.5371 - mean_squared_error: 965.8277 - mean_absolute_error: 12.3417 - val_loss: 1087.5646 - val_mean_squared_error: 1027.6769 - val_mean_absolute_error: 12.5649\n",
      "Epoch 27/50\n",
      "68504/68504 [==============================] - 7s 96us/step - loss: 1020.6216 - mean_squared_error: 960.0827 - mean_absolute_error: 12.3179 - val_loss: 1086.5510 - val_mean_squared_error: 1025.4366 - val_mean_absolute_error: 12.6101\n",
      "Epoch 28/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1016.1484 - mean_squared_error: 954.4575 - mean_absolute_error: 12.3042 - val_loss: 1083.0174 - val_mean_squared_error: 1021.3490 - val_mean_absolute_error: 12.2636\n",
      "Epoch 29/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 1010.6115 - mean_squared_error: 947.9364 - mean_absolute_error: 12.2862 - val_loss: 1086.5824 - val_mean_squared_error: 1024.0642 - val_mean_absolute_error: 11.9424\n",
      "Epoch 30/50\n",
      "68504/68504 [==============================] - 7s 107us/step - loss: 1006.3950 - mean_squared_error: 943.3023 - mean_absolute_error: 12.2316 - val_loss: 1086.6850 - val_mean_squared_error: 1023.0842 - val_mean_absolute_error: 12.8787\n",
      "Epoch 31/50\n",
      "68504/68504 [==============================] - 7s 102us/step - loss: 1002.7483 - mean_squared_error: 938.3999 - mean_absolute_error: 12.2344 - val_loss: 1080.3280 - val_mean_squared_error: 1015.8764 - val_mean_absolute_error: 12.4250\n",
      "Epoch 32/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 998.3388 - mean_squared_error: 933.1483 - mean_absolute_error: 12.1965 - val_loss: 1078.6600 - val_mean_squared_error: 1013.4510 - val_mean_absolute_error: 12.3351\n",
      "Epoch 33/50\n",
      "68504/68504 [==============================] - 7s 98us/step - loss: 994.5721 - mean_squared_error: 928.7629 - mean_absolute_error: 12.1510 - val_loss: 1080.2729 - val_mean_squared_error: 1014.1468 - val_mean_absolute_error: 12.7131\n",
      "Epoch 34/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 990.0201 - mean_squared_error: 923.4612 - mean_absolute_error: 12.1560 - val_loss: 1075.1053 - val_mean_squared_error: 1007.8763 - val_mean_absolute_error: 12.3556\n",
      "Epoch 35/50\n",
      "68504/68504 [==============================] - 7s 98us/step - loss: 986.0463 - mean_squared_error: 918.6549 - mean_absolute_error: 12.1145 - val_loss: 1077.0937 - val_mean_squared_error: 1009.1158 - val_mean_absolute_error: 12.5786\n",
      "Epoch 36/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 981.8874 - mean_squared_error: 913.0477 - mean_absolute_error: 12.1019 - val_loss: 1077.3453 - val_mean_squared_error: 1008.7183 - val_mean_absolute_error: 12.1280\n",
      "Epoch 37/50\n",
      "68504/68504 [==============================] - 7s 97us/step - loss: 979.2891 - mean_squared_error: 910.2020 - mean_absolute_error: 12.0627 - val_loss: 1075.2585 - val_mean_squared_error: 1005.6901 - val_mean_absolute_error: 12.2686\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 88)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 166)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_43 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           (None, 10000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 299)          0           input_45[0][0]                   \n",
      "                                                                 input_46[0][0]                   \n",
      "                                                                 input_47[0][0]                   \n",
      "                                                                 input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 32)           3232        input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 32)           320032      input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 32)           9600        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1)            33          dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            33          dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            33          dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3)            0           dense_25[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1)            4           concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 332,967\n",
      "Trainable params: 332,967\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 68504 samples, validate on 17126 samples\n",
      "Epoch 1/50\n",
      "68504/68504 [==============================] - 10s 147us/step - loss: 1620.5074 - mean_squared_error: 1594.2665 - mean_absolute_error: 17.8613 - val_loss: 1358.0732 - val_mean_squared_error: 1327.2209 - val_mean_absolute_error: 14.5163\n",
      "Epoch 2/50\n",
      "68504/68504 [==============================] - 9s 131us/step - loss: 1281.1696 - mean_squared_error: 1248.5133 - mean_absolute_error: 14.2324 - val_loss: 1261.8802 - val_mean_squared_error: 1228.2273 - val_mean_absolute_error: 13.6141\n",
      "Epoch 3/50\n",
      "68504/68504 [==============================] - 10s 139us/step - loss: 1221.9787 - mean_squared_error: 1186.5612 - mean_absolute_error: 13.7685 - val_loss: 1214.5855 - val_mean_squared_error: 1177.8586 - val_mean_absolute_error: 13.3876\n",
      "Epoch 4/50\n",
      "68504/68504 [==============================] - 10s 141us/step - loss: 1180.5652 - mean_squared_error: 1142.2306 - mean_absolute_error: 13.5114 - val_loss: 1183.8036 - val_mean_squared_error: 1144.9477 - val_mean_absolute_error: 13.2713\n",
      "Epoch 5/50\n",
      "68504/68504 [==============================] - 9s 133us/step - loss: 1147.9917 - mean_squared_error: 1107.8149 - mean_absolute_error: 13.3492 - val_loss: 1159.7260 - val_mean_squared_error: 1118.8530 - val_mean_absolute_error: 12.9760\n",
      "Epoch 6/50\n",
      "68504/68504 [==============================] - 9s 133us/step - loss: 1122.6703 - mean_squared_error: 1081.1252 - mean_absolute_error: 13.1750 - val_loss: 1139.5972 - val_mean_squared_error: 1097.3693 - val_mean_absolute_error: 13.1512\n",
      "Epoch 7/50\n",
      "68504/68504 [==============================] - 9s 133us/step - loss: 1101.2742 - mean_squared_error: 1058.4514 - mean_absolute_error: 13.1045 - val_loss: 1124.7359 - val_mean_squared_error: 1081.2782 - val_mean_absolute_error: 13.0266\n",
      "Epoch 8/50\n",
      "68504/68504 [==============================] - 9s 134us/step - loss: 1082.5990 - mean_squared_error: 1038.4842 - mean_absolute_error: 13.0367 - val_loss: 1108.7702 - val_mean_squared_error: 1064.4178 - val_mean_absolute_error: 12.8312\n",
      "Epoch 9/50\n",
      "68504/68504 [==============================] - 9s 134us/step - loss: 1067.2856 - mean_squared_error: 1022.1975 - mean_absolute_error: 12.9646 - val_loss: 1098.7510 - val_mean_squared_error: 1053.6329 - val_mean_absolute_error: 12.7183\n",
      "Epoch 10/50\n",
      "68504/68504 [==============================] - 10s 142us/step - loss: 1053.4440 - mean_squared_error: 1007.6381 - mean_absolute_error: 12.8808 - val_loss: 1090.4787 - val_mean_squared_error: 1044.4996 - val_mean_absolute_error: 12.9134\n",
      "Epoch 11/50\n",
      "68504/68504 [==============================] - 9s 134us/step - loss: 1041.9318 - mean_squared_error: 995.4425 - mean_absolute_error: 12.8199 - val_loss: 1082.6468 - val_mean_squared_error: 1035.4417 - val_mean_absolute_error: 12.7095\n",
      "Epoch 12/50\n",
      "68504/68504 [==============================] - 10s 146us/step - loss: 1032.3718 - mean_squared_error: 984.8277 - mean_absolute_error: 12.8344 - val_loss: 1073.4581 - val_mean_squared_error: 1025.9746 - val_mean_absolute_error: 12.7112\n",
      "Epoch 13/50\n",
      "68504/68504 [==============================] - 9s 135us/step - loss: 1022.7490 - mean_squared_error: 974.5453 - mean_absolute_error: 12.7600 - val_loss: 1069.6582 - val_mean_squared_error: 1021.7392 - val_mean_absolute_error: 12.6051\n",
      "Epoch 14/50\n",
      "68504/68504 [==============================] - 9s 133us/step - loss: 1014.0455 - mean_squared_error: 964.8874 - mean_absolute_error: 12.7544 - val_loss: 1065.6889 - val_mean_squared_error: 1016.9544 - val_mean_absolute_error: 12.8305\n",
      "Epoch 15/50\n",
      "68504/68504 [==============================] - 9s 134us/step - loss: 1007.3147 - mean_squared_error: 957.8722 - mean_absolute_error: 12.7183 - val_loss: 1062.6635 - val_mean_squared_error: 1013.4749 - val_mean_absolute_error: 12.4956\n",
      "Epoch 16/50\n",
      "68504/68504 [==============================] - 9s 132us/step - loss: 1000.7430 - mean_squared_error: 950.6913 - mean_absolute_error: 12.6847 - val_loss: 1057.0404 - val_mean_squared_error: 1007.3896 - val_mean_absolute_error: 12.8101\n",
      "Epoch 17/50\n",
      "68504/68504 [==============================] - 9s 133us/step - loss: 994.0190 - mean_squared_error: 943.0358 - mean_absolute_error: 12.7024 - val_loss: 1056.8627 - val_mean_squared_error: 1006.3562 - val_mean_absolute_error: 12.7766\n",
      "Epoch 18/50\n",
      "68504/68504 [==============================] - 9s 131us/step - loss: 988.7977 - mean_squared_error: 937.3887 - mean_absolute_error: 12.6372 - val_loss: 1054.5964 - val_mean_squared_error: 1003.2765 - val_mean_absolute_error: 12.7287\n",
      "Epoch 19/50\n",
      "68504/68504 [==============================] - 9s 132us/step - loss: 983.4040 - mean_squared_error: 931.4314 - mean_absolute_error: 12.6260 - val_loss: 1051.3364 - val_mean_squared_error: 999.4248 - val_mean_absolute_error: 12.6509\n",
      "Epoch 20/50\n",
      "68504/68504 [==============================] - 9s 134us/step - loss: 977.4695 - mean_squared_error: 924.7892 - mean_absolute_error: 12.6443 - val_loss: 1052.5889 - val_mean_squared_error: 1000.4377 - val_mean_absolute_error: 12.5680\n",
      "Epoch 21/50\n",
      "68504/68504 [==============================] - 11s 153us/step - loss: 971.7816 - mean_squared_error: 918.4206 - mean_absolute_error: 12.5518 - val_loss: 1048.4999 - val_mean_squared_error: 995.6087 - val_mean_absolute_error: 12.5217\n",
      "Epoch 22/50\n",
      "68504/68504 [==============================] - 9s 138us/step - loss: 967.1974 - mean_squared_error: 913.0701 - mean_absolute_error: 12.5422 - val_loss: 1048.2070 - val_mean_squared_error: 994.1616 - val_mean_absolute_error: 12.4705\n",
      "Epoch 23/50\n",
      "68504/68504 [==============================] - 9s 135us/step - loss: 962.0197 - mean_squared_error: 907.2399 - mean_absolute_error: 12.5088 - val_loss: 1049.8252 - val_mean_squared_error: 994.8998 - val_mean_absolute_error: 12.6199\n",
      "Epoch 24/50\n",
      "68504/68504 [==============================] - 9s 135us/step - loss: 956.9457 - mean_squared_error: 901.2535 - mean_absolute_error: 12.4796 - val_loss: 1046.9199 - val_mean_squared_error: 991.2340 - val_mean_absolute_error: 12.3972\n",
      "Epoch 25/50\n",
      "68504/68504 [==============================] - 9s 134us/step - loss: 951.8369 - mean_squared_error: 895.1735 - mean_absolute_error: 12.4584 - val_loss: 1047.3903 - val_mean_squared_error: 991.0839 - val_mean_absolute_error: 12.4286\n",
      "Epoch 26/50\n",
      "68504/68504 [==============================] - 9s 132us/step - loss: 945.5335 - mean_squared_error: 888.2971 - mean_absolute_error: 12.3575 - val_loss: 1050.7563 - val_mean_squared_error: 993.6489 - val_mean_absolute_error: 13.1305\n",
      "Epoch 27/50\n",
      "68504/68504 [==============================] - 9s 132us/step - loss: 942.1114 - mean_squared_error: 883.1712 - mean_absolute_error: 12.5598 - val_loss: 1050.2763 - val_mean_squared_error: 992.1335 - val_mean_absolute_error: 12.7465\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_54 (InputLayer)           (None, 136)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           (None, 88)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_51 (InputLayer)           (None, 166)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_52 (InputLayer)           (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_53 (InputLayer)           (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 136, 100)     3099000     input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_49 (InputLayer)           (None, 10000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 299)          0           input_50[0][0]                   \n",
      "                                                                 input_51[0][0]                   \n",
      "                                                                 input_52[0][0]                   \n",
      "                                                                 input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 32)           17152       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 32)           320032      input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 32)           9600        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1)            33          cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1)            33          dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            33          dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 3)            0           dense_31[0][0]                   \n",
      "                                                                 dense_33[0][0]                   \n",
      "                                                                 dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 1)            4           concatenate_12[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 3,445,887\n",
      "Trainable params: 346,887\n",
      "Non-trainable params: 3,099,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 68504 samples, validate on 17126 samples\n",
      "Epoch 1/50\n",
      "68504/68504 [==============================] - 18s 268us/step - loss: 1632.1966 - mean_squared_error: 1616.2455 - mean_absolute_error: 17.6713 - val_loss: 1354.6494 - val_mean_squared_error: 1334.1903 - val_mean_absolute_error: 14.4980\n",
      "Epoch 2/50\n",
      "68504/68504 [==============================] - 17s 242us/step - loss: 1277.7926 - mean_squared_error: 1255.1280 - mean_absolute_error: 14.2415 - val_loss: 1260.0669 - val_mean_squared_error: 1235.5710 - val_mean_absolute_error: 13.9161\n",
      "Epoch 3/50\n",
      "68504/68504 [==============================] - 16s 237us/step - loss: 1218.3278 - mean_squared_error: 1191.5849 - mean_absolute_error: 13.8586 - val_loss: 1211.2792 - val_mean_squared_error: 1183.3277 - val_mean_absolute_error: 13.3904\n",
      "Epoch 4/50\n",
      "68504/68504 [==============================] - 17s 243us/step - loss: 1179.0432 - mean_squared_error: 1149.4260 - mean_absolute_error: 13.5521 - val_loss: 1181.4982 - val_mean_squared_error: 1150.9827 - val_mean_absolute_error: 13.3379\n",
      "Epoch 5/50\n",
      "68504/68504 [==============================] - 17s 246us/step - loss: 1148.8181 - mean_squared_error: 1117.0261 - mean_absolute_error: 13.3832 - val_loss: 1158.9108 - val_mean_squared_error: 1126.2824 - val_mean_absolute_error: 13.1993\n",
      "Epoch 6/50\n",
      "68504/68504 [==============================] - 18s 256us/step - loss: 1123.9574 - mean_squared_error: 1090.5650 - mean_absolute_error: 13.2303 - val_loss: 1140.2203 - val_mean_squared_error: 1106.3168 - val_mean_absolute_error: 13.1357\n",
      "Epoch 7/50\n",
      "68504/68504 [==============================] - 17s 243us/step - loss: 1103.3465 - mean_squared_error: 1068.7740 - mean_absolute_error: 13.1429 - val_loss: 1126.0725 - val_mean_squared_error: 1090.9185 - val_mean_absolute_error: 13.0273\n",
      "Epoch 8/50\n",
      "68504/68504 [==============================] - 16s 236us/step - loss: 1085.9803 - mean_squared_error: 1050.4126 - mean_absolute_error: 13.0365 - val_loss: 1117.7260 - val_mean_squared_error: 1081.3681 - val_mean_absolute_error: 13.3145\n",
      "Epoch 9/50\n",
      "68504/68504 [==============================] - 17s 251us/step - loss: 1073.3026 - mean_squared_error: 1036.4533 - mean_absolute_error: 13.0000 - val_loss: 1105.5418 - val_mean_squared_error: 1068.3388 - val_mean_absolute_error: 13.0019\n",
      "Epoch 10/50\n",
      "68504/68504 [==============================] - 17s 242us/step - loss: 1060.8517 - mean_squared_error: 1023.0903 - mean_absolute_error: 12.9372 - val_loss: 1101.1493 - val_mean_squared_error: 1062.6379 - val_mean_absolute_error: 13.0287\n",
      "Epoch 11/50\n",
      "68504/68504 [==============================] - 17s 254us/step - loss: 1050.3366 - mean_squared_error: 1011.4852 - mean_absolute_error: 12.9033 - val_loss: 1088.8389 - val_mean_squared_error: 1049.9479 - val_mean_absolute_error: 12.8642\n",
      "Epoch 12/50\n",
      "68504/68504 [==============================] - 16s 237us/step - loss: 1041.8444 - mean_squared_error: 1002.2503 - mean_absolute_error: 12.8779 - val_loss: 1081.3559 - val_mean_squared_error: 1041.2891 - val_mean_absolute_error: 12.7580\n",
      "Epoch 13/50\n",
      "68504/68504 [==============================] - 16s 237us/step - loss: 1033.2720 - mean_squared_error: 992.6994 - mean_absolute_error: 12.8358 - val_loss: 1081.9380 - val_mean_squared_error: 1041.2925 - val_mean_absolute_error: 12.7233\n",
      "Epoch 14/50\n",
      "68504/68504 [==============================] - 16s 236us/step - loss: 1026.0153 - mean_squared_error: 984.6967 - mean_absolute_error: 12.7979 - val_loss: 1074.0872 - val_mean_squared_error: 1032.6756 - val_mean_absolute_error: 12.7390\n",
      "Epoch 15/50\n",
      "68504/68504 [==============================] - 17s 251us/step - loss: 1018.5975 - mean_squared_error: 976.4200 - mean_absolute_error: 12.7803 - val_loss: 1072.4552 - val_mean_squared_error: 1030.2090 - val_mean_absolute_error: 12.6591\n",
      "Epoch 16/50\n",
      "68504/68504 [==============================] - 17s 254us/step - loss: 1012.8857 - mean_squared_error: 970.2798 - mean_absolute_error: 12.7470 - val_loss: 1070.2458 - val_mean_squared_error: 1027.2364 - val_mean_absolute_error: 12.9697\n",
      "Epoch 17/50\n",
      "68504/68504 [==============================] - 16s 235us/step - loss: 1006.6293 - mean_squared_error: 963.3172 - mean_absolute_error: 12.7445 - val_loss: 1069.4876 - val_mean_squared_error: 1025.8529 - val_mean_absolute_error: 13.1317\n",
      "Epoch 18/50\n",
      "68504/68504 [==============================] - 16s 238us/step - loss: 999.9842 - mean_squared_error: 955.6396 - mean_absolute_error: 12.7652 - val_loss: 1062.3293 - val_mean_squared_error: 1018.1007 - val_mean_absolute_error: 12.7655\n",
      "Epoch 19/50\n",
      "68504/68504 [==============================] - 16s 239us/step - loss: 994.9691 - mean_squared_error: 950.2164 - mean_absolute_error: 12.6868 - val_loss: 1058.3279 - val_mean_squared_error: 1013.3587 - val_mean_absolute_error: 12.8900\n",
      "Epoch 20/50\n",
      "68504/68504 [==============================] - 17s 246us/step - loss: 989.3432 - mean_squared_error: 943.7686 - mean_absolute_error: 12.7143 - val_loss: 1059.2551 - val_mean_squared_error: 1013.8052 - val_mean_absolute_error: 12.7284\n",
      "Epoch 21/50\n",
      "68504/68504 [==============================] - 17s 246us/step - loss: 985.2845 - mean_squared_error: 939.0131 - mean_absolute_error: 12.6960 - val_loss: 1054.2796 - val_mean_squared_error: 1008.3974 - val_mean_absolute_error: 12.6525\n",
      "Epoch 22/50\n",
      "68504/68504 [==============================] - 16s 238us/step - loss: 981.0230 - mean_squared_error: 934.6969 - mean_absolute_error: 12.6418 - val_loss: 1065.4893 - val_mean_squared_error: 1018.7608 - val_mean_absolute_error: 13.3968\n",
      "Epoch 23/50\n",
      "68504/68504 [==============================] - 16s 238us/step - loss: 976.2677 - mean_squared_error: 929.0212 - mean_absolute_error: 12.7172 - val_loss: 1053.1894 - val_mean_squared_error: 1005.9936 - val_mean_absolute_error: 12.6972\n",
      "Epoch 24/50\n",
      "68504/68504 [==============================] - 16s 237us/step - loss: 972.9520 - mean_squared_error: 925.2679 - mean_absolute_error: 12.6517 - val_loss: 1053.9464 - val_mean_squared_error: 1006.1371 - val_mean_absolute_error: 12.7872\n",
      "Epoch 25/50\n",
      "68504/68504 [==============================] - 17s 244us/step - loss: 968.5429 - mean_squared_error: 920.1260 - mean_absolute_error: 12.6479 - val_loss: 1051.3211 - val_mean_squared_error: 1003.1611 - val_mean_absolute_error: 13.0491\n",
      "Epoch 26/50\n",
      "68504/68504 [==============================] - 17s 242us/step - loss: 964.0606 - mean_squared_error: 915.3448 - mean_absolute_error: 12.6110 - val_loss: 1043.0853 - val_mean_squared_error: 993.9213 - val_mean_absolute_error: 12.7471\n",
      "Epoch 27/50\n",
      "68504/68504 [==============================] - 17s 249us/step - loss: 957.8864 - mean_squared_error: 908.0870 - mean_absolute_error: 12.5584 - val_loss: 1068.2601 - val_mean_squared_error: 1018.9512 - val_mean_absolute_error: 13.8367\n",
      "Epoch 28/50\n",
      "68504/68504 [==============================] - 17s 243us/step - loss: 956.8963 - mean_squared_error: 906.5753 - mean_absolute_error: 12.6164 - val_loss: 1042.6646 - val_mean_squared_error: 991.8016 - val_mean_absolute_error: 12.7213\n",
      "Epoch 29/50\n",
      "68504/68504 [==============================] - 16s 238us/step - loss: 951.3626 - mean_squared_error: 900.2905 - mean_absolute_error: 12.5264 - val_loss: 1055.9725 - val_mean_squared_error: 1004.6326 - val_mean_absolute_error: 13.0474\n",
      "Epoch 30/50\n",
      "68504/68504 [==============================] - 17s 255us/step - loss: 947.8141 - mean_squared_error: 895.7365 - mean_absolute_error: 12.5687 - val_loss: 1044.5433 - val_mean_squared_error: 992.7786 - val_mean_absolute_error: 12.9970\n",
      "Epoch 31/50\n",
      "68504/68504 [==============================] - 16s 239us/step - loss: 943.7998 - mean_squared_error: 891.3981 - mean_absolute_error: 12.5310 - val_loss: 1039.5111 - val_mean_squared_error: 986.9730 - val_mean_absolute_error: 12.8204\n",
      "Epoch 32/50\n",
      "68504/68504 [==============================] - 16s 238us/step - loss: 939.7791 - mean_squared_error: 886.3520 - mean_absolute_error: 12.5612 - val_loss: 1042.7332 - val_mean_squared_error: 989.7855 - val_mean_absolute_error: 12.7405\n",
      "Epoch 33/50\n",
      "68504/68504 [==============================] - 16s 234us/step - loss: 936.7032 - mean_squared_error: 883.1525 - mean_absolute_error: 12.5122 - val_loss: 1043.8684 - val_mean_squared_error: 990.1445 - val_mean_absolute_error: 12.7477\n",
      "Epoch 34/50\n",
      "68504/68504 [==============================] - 18s 257us/step - loss: 932.1870 - mean_squared_error: 877.7726 - mean_absolute_error: 12.4857 - val_loss: 1048.4160 - val_mean_squared_error: 994.2731 - val_mean_absolute_error: 12.9529\n"
     ]
    }
   ],
   "source": [
    "model_predictions=[]\n",
    "model_history=[]\n",
    "\n",
    "for i in np.arange(1,4):\n",
    "  predictions, history = train_and_evaluate(i)\n",
    "  model_predictions.append(predictions)\n",
    "  model_history.append(history)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zN6re7Kkovvq"
   },
   "source": [
    "## Results\n",
    "\n",
    "*  Plots of the training and validation losses seem to be well in term of fitting and the models seem to have fit its available capacity\n",
    "*  Some random sample test predictions of all the three model are printed\n",
    "*  It seems the models seemed to perform similary in terms of MSE and MEA. \n",
    "*  When the tf-idf features were not used, the models seemed to perform slighlty worse\n",
    "\n",
    "## Possible Improvements\n",
    "\n",
    "* LSTM layer could be changed with Bi-Directional LSTM\n",
    "* Different neural architecture such as CNN could have been tried if there was more time available\n",
    "* Winery feature could also be incorporated to the models and evaluations could be done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2146
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1071833,
     "status": "ok",
     "timestamp": 1557087201602,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "sygFKog8lg1a",
    "outputId": "07e6a6d8-cfd4-4b62-ca7e-92877a8b56ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model:  1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:  1141.995272802972\n",
      "Test MAE:  12.899635494178542 \n",
      "\n",
      "Results for model:  2 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8lOW9///XJ/u+hyULBBDZVwGt\nS8WjRdRatO7VVq2V1mO/tr/TRW1Pazd7PG2Px7Zaq1as9iiUalVatVYriNaFxQKyCrIlLElIWLKv\n1++P+06YQAIJZDJJ5v18POYxM9d9zz3XzTzgzbXc123OOURERDorItQVEBGRvkXBISIiXaLgEBGR\nLlFwiIhIlyg4RESkSxQcIiLSJQoOkW5mZr83s590ct/tZnbByR5HpCcpOEREpEsUHCIi0iUKDglL\nfhfRt8xsjZlVmdnjZjbQzF4xswoze93M0gP2/4yZrTOzA2a2xMzGBGybYmYf+J/7IxB3xHd92sxW\n+Z99x8wmnmCdbzWzLWZWbmaLzCzHLzcz+18zKzGzQ2b2oZmN97ddbGbr/brtMrNvntAfmEgABYeE\nsyuATwGnApcCrwDfAbLx/m7cAWBmpwLzga/7214G/mJmMWYWA7wA/AHIAP7kHxf/s1OAecCXgUzg\nEWCRmcV2paJm9m/AfwFXA4OBHcACf/Ms4JP+eaT6+5T52x4HvuycSwbGA2905XtF2qPgkHD2a+dc\nsXNuF/AW8L5z7l/OuVrgeWCKv981wEvOudeccw3AL4B44EzgDCAaeMA51+CcexZYHvAdc4FHnHPv\nO+eanHNPAnX+57riemCec+4D51wdcDfwCTMrABqAZGA0YM65Dc65Pf7nGoCxZpbinNvvnPugi98r\nchQFh4Sz4oDXNe28T/Jf5+D9Dx8A51wzUAjk+tt2ubarhe4IeD0U+IbfTXXAzA4A+f7nuuLIOlTi\ntSpynXNvAA8CDwElZvaomaX4u14BXAzsMLM3zewTXfxekaMoOESObzdeAADemALeP/67gD1Arl/W\nYkjA60LgXudcWsAjwTk3/yTrkIjX9bULwDn3K+fcacBYvC6rb/nly51zc4ABeF1qC7v4vSJHUXCI\nHN9C4BIzO9/MooFv4HU3vQO8CzQCd5hZtJl9FpgR8NnHgK+Y2en+IHaimV1iZsldrMN84GYzm+yP\nj/wUr2ttu5lN948fDVQBtUCzPwZzvZml+l1sh4Dmk/hzEAEUHCLH5ZzbBNwA/BrYhzeQfqlzrt45\nVw98FrgJKMcbD/lzwGdXALfidSXtB7b4+3a1Dq8D3wOew2vljACu9Ten4AXUfrzurDLg5/62zwPb\nzewQ8BW8sRKRk2K6kZOIiHSFWhwiItIlCg4REekSBYeIiHSJgkNERLokKtQVCIasrCxXUFAQ6mqI\niPQpK1eu3Oecyz7efv0yOAoKClixYkWoqyEi0qeY2Y7j76WuKhER6SIFh4iIdImCQ0REuqRfjnG0\np6GhgaKiImpra0NdlX4jLi6OvLw8oqOjQ10VEelBYRMcRUVFJCcnU1BQQNuFTOVEOOcoKyujqKiI\nYcOGhbo6ItKDwqarqra2lszMTIVGNzEzMjMz1YITCUNhExyAQqOb6c9TJDyFVXAcT2NzM8WHaqmu\nbwx1VUREei0FRwADig/VUlUXnOA4cOAAv/nNb7r8uYsvvpgDBw4EoUYiIl2n4AgQGRFBpBkNTcG5\nR0lHwdHYeOygevnll0lLSwtKnUREuipsZlV1VnRUBPWNwbm75l133cXHH3/M5MmTiY6OJi4ujvT0\ndDZu3MhHH33EZZddRmFhIbW1tXzta19j7ty5wOElVCorK7nooos4++yzeeedd8jNzeXFF18kPj4+\nKPUVEWlP0ILDzOYBnwZKnHPjA8r/H3A70AS85Jz7tl9+N3CLX36Hc+5Vv3w28EsgEvidc+6+k63b\nD/+yjvW7D7W7rbahCQfER0d26Zhjc1K459Jxx9znvvvuY+3ataxatYolS5ZwySWXsHbt2tbprPPm\nzSMjI4OamhqmT5/OFVdcQWZmZptjbN68mfnz5/PYY49x9dVX89xzz3HDDTd0qa4iIicjmC2O3+Pd\nZ/mplgIzOw+YA0xyztWZ2QC/fCze/ZPHATnA62Z2qv+xh4BPAUXAcjNb5JxbH6xKR5jR2BycFseR\nZsyY0eYaiF/96lc8//zzABQWFrJ58+ajgmPYsGFMnjwZgNNOO43t27f3SF1FRFoELTicc0vNrOCI\n4tuA+5xzdf4+JX75HGCBX77NzLYAM/xtW5xzWwHMbIG/70kFx7FaBiUVtew9WMu4nFQiI4I73TQx\nMbH19ZIlS3j99dd59913SUhIYObMme1eIxEbG9v6OjIykpqamqDWUUTkSD09OH4qcI6ZvW9mb5rZ\ndL88FygM2K/IL+uo/ChmNtfMVpjZitLS0hOuYEyk90fS0NT9rY7k5GQqKira3Xbw4EHS09NJSEhg\n48aNvPfee93+/SIi3aGnB8ejgAzgDGA6sNDMhnfHgZ1zjwKPAkybNu2Ep0VFBwRHXBfHOY4nMzOT\ns846i/HjxxMfH8/AgQNbt82ePZvf/va3jBkzhlGjRnHGGWd063eLiHSXng6OIuDPzjkHLDOzZiAL\n2AXkB+yX55dxjPKgaAmOYM2seuaZZ9otj42N5ZVXXml3W8s4RlZWFmvXrm0t/+Y3v9nt9RMROZ6e\n7qp6ATgPwB/8jgH2AYuAa80s1syGASOBZcByYKSZDTOzGLwB9EXBrGB0pGEE71oOEZG+LpjTcecD\nM4EsMysC7gHmAfPMbC1QD9zotz7WmdlCvEHvRuB251yTf5yvAq/iTced55xbF6w6+99HdKQFZYxD\nRKQ/COasqus62NTuRQfOuXuBe9spfxl4uRurdlzRkRHUKzhERNqlJUfaERMVQUOQxjhERPo6BUc7\nvK4qh9eLJiIigRQc7YiOjMDhNEAuItIOBUc7YqKCdxFgVyQlJQGwe/durrzyynb3mTlzJitWrDjm\ncR544AGqq6tb32uZdhE5GQqOdkQH8erxE5GTk8Ozzz57wp8/Mji0TLuInAwFRztaLwLs5uC46667\neOihh1rf/+AHP+AnP/kJ559/PlOnTmXChAm8+OKLR31u+/btjB/vLTBcU1PDtddey5gxY7j88svb\nrFV12223MW3aNMaNG8c999wDeAsn7t69m/POO4/zzjsP8JZp37dvHwD3338/48ePZ/z48TzwwAOt\n3zdmzBhuvfVWxo0bx6xZs7Qmloi0Cs/7cbxyF+z9sMPNkcCI+kaiIgyiOrnsyKAJcNGxV3y/5ppr\n+PrXv87tt98OwMKFC3n11Ve54447SElJYd++fZxxxhl85jOf6fB+3g8//DAJCQls2LCBNWvWMHXq\n1NZt9957LxkZGTQ1NXH++eezZs0a7rjjDu6//34WL15MVlZWm2OtXLmSJ554gvfffx/nHKeffjrn\nnnsu6enpWr5dRDqkFkcHzKC7J1VNmTKFkpISdu/ezerVq0lPT2fQoEF85zvfYeLEiVxwwQXs2rWL\n4uLiDo+xdOnS1n/AJ06cyMSJE1u3LVy4kKlTpzJlyhTWrVvH+vXHXkT47bff5vLLLycxMZGkpCQ+\n+9nP8tZbbwFavl1EOhaeLY7jtAwASvZVUd/UzKkDk7v1q6+66iqeffZZ9u7dyzXXXMPTTz9NaWkp\nK1euJDo6moKCgnaXUz+ebdu28Ytf/ILly5eTnp7OTTfddELHaaHl20WkI2pxdCA6KiIog+PXXHMN\nCxYs4Nlnn+Wqq67i4MGDDBgwgOjoaBYvXsyOHTuO+flPfvKTrQslrl27ljVr1gBw6NAhEhMTSU1N\npbi4uM2CiR0t537OOefwwgsvUF1dTVVVFc8//zznnHNON56tiPRH4dni6ISYSKOp2dHU3ExkRPfl\n67hx46ioqCA3N5fBgwdz/fXXc+mllzJhwgSmTZvG6NGjj/n52267jZtvvpkxY8YwZswYTjvtNAAm\nTZrElClTGD16NPn5+Zx11lmtn5k7dy6zZ88mJyeHxYsXt5ZPnTqVm266iRkzvHtmfelLX2LKlCnq\nlhKRY7L+eHX0tGnT3JHXNmzYsIExY8Z0+hgHquvZWV7NyIHJXb7/eDjp6p+riPReZrbSOTftePup\nq6oDrddyaM0qEZE2FBwd6C1Xj4uI9DZhFRxd6ZaLijDMTMurH0N/7OYUkeMLm+CIi4ujrKys0//Y\ntd7QqVH/OLbHOUdZWRlxcXGhroqI9LCwmVWVl5dHUVERpaWlnf5MaUUdAFUlscfZMzzFxcWRl5cX\n6mqISA8Lm+CIjo5m2LBhXfrM7xau5p2P9/Hu3ecHqVYiIn1P2HRVnYjctDiKD9VqgFxEJICC4xhy\n0uJpdlB86MSX7hAR6W+CFhxmNs/MSsxsbUDZD8xsl5mt8h8XB2y728y2mNkmM7swoHy2X7bFzO4K\nVn3bk5seD8Cu/VqnSUSkRTBbHL8HZrdT/r/Oucn+42UAMxsLXAuM8z/zGzOLNLNI4CHgImAscJ2/\nb4/ISfOCY/dBBYeISIugDY4755aaWUEnd58DLHDO1QHbzGwLMMPftsU5txXAzBb4+x57vfBukpPq\nB8cBdVWJiLQIxRjHV81sjd+Vle6X5QKFAfsU+WUdlR/FzOaa2QozW9GVKbfHEh8TSWZiDEXqqhIR\nadXTwfEwMAKYDOwB/qe7Duyce9Q5N805Ny07O7u7DktOWjy7Dyg4RERa9GhwOOeKnXNNzrlm4DEO\nd0ftAvIDds3zyzoq7zE5aXEKDhGRAD0aHGY2OODt5UDLjKtFwLVmFmtmw4CRwDJgOTDSzIaZWQze\nAPqinqxzbloCuw/UaF0mERFf0AbHzWw+MBPIMrMi4B5gpplNBhywHfgygHNunZktxBv0bgRud841\n+cf5KvAqEAnMc86tC1ad25OTFkdVfRMHaxpIS4jpya8WEemVgjmr6rp2ih8/xv73Ave2U/4y8HI3\nVq1Lcv0pubsO1Cg4RETQlePH1XIRoKbkioh4FBzH0XIR4K791SGuiYhI76DgOI7MxBhioiLYfVAt\nDhERUHAcl5mRmxbPLk3JFREBFBydkpsWr4UORUR8Co5O0EWAIiKHKTg6ISctnpKKOuoam0JdFRGR\nkFNwdELLtRx7NUAuIqLg6IzAiwBFRMKdgqMTWm/opIsARUQUHJ0xOC0OQAPkIiIoODolNiqS7ORY\nTckVEUHB0Wk5afG697iICAqOTsvT1eMiIoCCo9NaLgLUDZ1EJNwpODopJy2e2oZmyqvqQ10VEZGQ\nUnB0kqbkioh4FByddPgiQN2XQ0TCm4Kjkw4Hh1ocIhLeFBydlJYQTXx0pC4CFJGwF7TgMLN5ZlZi\nZmvb2fYNM3NmluW/NzP7lZltMbM1ZjY1YN8bzWyz/7gxWPU9HjMjN1335RARCWaL4/fA7CMLzSwf\nmAXsDCi+CBjpP+YCD/v7ZgD3AKcDM4B7zCw9iHU+Jl0EKCISxOBwzi0FytvZ9L/At4HACyLmAE85\nz3tAmpkNBi4EXnPOlTvn9gOv0U4Y9ZRc3dBJRKRnxzjMbA6wyzm3+ohNuUBhwPsiv6yj8vaOPdfM\nVpjZitLS0m6sdUAl0+LZV1lPbYNu6CQi4avHgsPMEoDvAN8PxvGdc48656Y556ZlZ2cH4ysCruVQ\nq0NEwldPtjhGAMOA1Wa2HcgDPjCzQcAuID9g3zy/rKPykNBFgCIiPRgczrkPnXMDnHMFzrkCvG6n\nqc65vcAi4Av+7KozgIPOuT3Aq8AsM0v3B8Vn+WUhkasWh4hIUKfjzgfeBUaZWZGZ3XKM3V8GtgJb\ngMeAfwdwzpUDPwaW+48f+WUhMSg1DjMoUnCISBiLCtaBnXPXHWd7QcBrB9zewX7zgHndWrkTFB0Z\nwcBkzawSkfCmK8e7KEdTckUkzCk4uig3PUE3dBKRsKbg6KKctDj2HKiluVk3dBKR8KTg6KLctHjq\nm5rZV1UX6qqIiISEgqOLWpdX12KHIhKmFBxHqi6Hho5DQRcBiki4U3AEKvsY7h8DaxZ2uIuWHRGR\ncKfgCJQxHDJGwPLHwLU/+J0aH01ybJRmVolI2FJwBDKDGV+CvR9C4bIOd8tJi1dwiEjYUnAcacLV\nEJvitTo6oIsARSScKTiOFJsEkz8H616AypJ2d8lJi1dwiEjYUnC0Z/qXoLkBPniy3c256fHsr26g\nur6xhysmIhJ6Co72ZI2E4TNhxRPQdHQ4aHl1EQlnCo6OTL8VDu2Cj145alPLlNxdupZDRMKQgqMj\np86GlDxYdvQgua4eF5FwpuDoSGQUTLsZtr0JpR+12TQgOZbICFNXlYiEJQXHsUy9ESJjYPnv2hRH\nRUYwKEVTckUkPCk4jiUpG8ZeBqvnQ11lm025afG6hayIhCUFx/HMuBXqDsGaP7Yp1kWAIhKuFBzH\nkzcdBk3wuqsC1q/KSYtn78FamnRDJxEJM0ELDjObZ2YlZrY2oOzHZrbGzFaZ2d/NLMcvNzP7lZlt\n8bdPDfjMjWa22X/cGKz6HuNEvKm5Jeth57utxTlp8TQ2O0oqNCVXRMJLMFscvwdmH1H2c+fcROfc\nZOCvwPf98ouAkf5jLvAwgJllAPcApwMzgHvMLD2IdW7fhKsgLrXN1NzcdF0EKCLhKWjB4ZxbCpQf\nUXYo4G0i0NLPMwd4ynneA9LMbDBwIfCac67cObcfeI2jwyj4YhJg8g2wYRFU7AUCruXQRYAiEmY6\nFRxm9jUzS/G7lB43sw/MbNaJfKGZ3WtmhcD1HG5x5AKFAbsV+WUdlfe86bdAcyOs9Nav0g2dRCRc\ndbbF8UW/tTALSAc+D9x3Il/onPuucy4feBr46okcoz1mNtfMVpjZitLS0u467GGZI2DE+bDyCWhq\nICk2itT4aF09LiJhp7PBYf7zxcAfnHPrAspO1NPAFf7rXUB+wLY8v6yj8qM45x51zk1zzk3Lzs4+\nyap1YMatULEHNr4EaHl1EQlPnQ2OlWb2d7zgeNXMkoHmrn6ZmY0MeDsH2Oi/XgR8we8KOwM46Jzb\nA7wKzDKzdH9QfJZfFhojZ0HqkNYryXPT4nQnQBEJO1Gd3O8WYDKw1TlX7c92uvlYHzCz+cBMIMvM\nivBmR11sZqPwQmcH8BV/95fxQmkLUN1ybOdcuZn9GFju7/cj51ybAfceFREJ078Ir/8ASjaQmxbP\n+9tCVx0RkVDobHB8AljlnKsysxuAqcAvj/UB59x17RQ/3sG+Dri9g23zgHmdrGfwTfkCLP4vWP47\nctL+nYraRg7WNJAaHx3qmomI9IjOdlU9DFSb2STgG8DHwFNBq1VvlpgJ4z8LqxcwfbCXu79ZsiXE\nlRIR6TmdDY5Gv1UwB3jQOfcQkBy8avVy02+F+kqmHvg7180YwqNLt7Jyh7qsRCQ8dDY4Kszsbrxp\nuC+ZWQQQvn0zuVNh8GRY9hjfvXg0uWnxfGPhat2DXETCQmeD4xqgDu96jr1402J/HrRa9XZm3tTc\nfZtI2vMuP79yEtvLqvnvVzYe/7MiIn1cp4LDD4ungVQz+zRQ65wLzzGOFuOvgPh0WPYYnxiRyc1n\nFfDkuzt4Z8u+UNdMRCSoOrvkyNXAMuAq4GrgfTO7MpgV6/Wi42HKDd7FgLv/xbcvHM3wrES+9ewa\nKmobQl07EZGg6WxX1XeB6c65G51zX8BbqfZ7watWH3Hm1yAlF565lviavfzi6knsOVjDT/66IdQ1\nExEJms4GR4RzriTgfVkXPtt/JWXD5/4I9VUw/xqmDozmy+eO4I8rClm8seT4nxcR6YM6+4//38zs\nVTO7ycxuAl7Cu9pbBo6Fq34PxevguS/x9X8bzqiBydz53BoOVNeHunYiIt2us4Pj3wIeBSb6j0ed\nc3cGs2J9ysgL4KKfwUevELv4h/zP1ZMor6rnnkXrQl0zEZFu19klR3DOPQc8F8S69G0zboV9m+Hd\nBxmfOYI7zj+H+1/7iNnjBnHRhMGhrp2ISLc5ZovDzCrM7FA7jwozO3Ssz4alC3/qraD70jf59yE7\nmZCbyndfWMu+yrpQ10xEpNscMzicc8nOuZR2HsnOuZSeqmSfERkFV86D7NFEPXsTv74gnsq6Rr7z\n5w/xVmwREen7NDOqu8UmezOtouIoePVm/nNmFn9fX8wLq9q9/5SISJ+j4AiGtHy4bgFUFvP57d/h\nE0MS+f6L69h7sDbUNRMROWkKjmDJOw0ufwQrWsZjqU/Q2NTMnc+tUZeViPR5Co5gGncZnP99kja/\nwPxT3+TNj0qZv6ww1LUSETkpnZ6OKyfo7P+AfVuYvPphvpWTyg8WRZCTFsfMUQNCXTMRkROiFkew\nmcGlv4ShZ/HvB+/nMxk7+fIfVvJPraIrIn2UgqMnRMXANf+Hpebx89of8oXkD/jSkytYtk13DRSR\nvkfB0VMSMuCml7BB4/luzc+4N+4p5j7xDh/s3B/qmomIdEnQgsPM5plZiZmtDSj7uZltNLM1Zva8\nmaUFbLvbzLaY2SYzuzCgfLZftsXM7gpWfXtESg7c9BJ84qt8tuElno76IXfOe5kPiw6GumYiIp0W\nzBbH74HZR5S9Box3zk0EPgLuBjCzscC1wDj/M78xs0gziwQeAi4CxgLX+fv2XZHRcOG9cPVTjIna\nzbN8m4cf/y0b9mgFFxHpG4IWHM65pUD5EWV/d841+m/fw7t3OcAcYIFzrs45tw3YgnezqBnAFufc\nVudcPbDA37fvGzuHiC8vJT5rCA+6/+KtR/+DzXsOhLpWIiLHFcoxji8Cr/ivc4HACxyK/LKOyo9i\nZnPNbIWZrSgtLQ1CdYMgcwQxc/9B1eirmOv+RPmjl7KjcGeoayUickwhCQ4z+y7QCDzdXcd0zj3q\nnJvmnJuWnZ3dXYcNvpgEkq99jOKZv2CS20j84+dSvO7NUNdKRKRDPR4c/h0EPw1c7w6vv7ELyA/Y\nLc8v66i83xk481Z2X7GIWqLJ/NPlHHjjAdDyJCLSC/VocJjZbODbwGecc9UBmxYB15pZrJkNA0YC\ny4DlwEgzG2ZmMXgD6It6ss49afiET1Dx+dd5000lbek91D5zA9RqxpWI9C7BnI47H3gXGGVmRWZ2\nC/AgkAy8ZmarzOy3AM65dcBCYD3wN+B251yTP5D+VeBVYAOw0N+33xo3YgjpX1zIz5pvIHrzyzT9\nehqs/qNaHyLSa1h/XK112rRpbsWKFaGuxklZtq2cn82bz4+i5jHWbYEhZ8Ilv4CB40JdNRHpp8xs\npXNu2vH205XjvdSMYRl8b+713BJ1H99vnkv93vXw23Pgb3er+0pEQkrB0YtNyk/jxTvOYd3gy5lx\n6L/5V/Yc3HsPw6+nweoF6r4SkZBQcPRyA5LjeObW07loxlgu33klPx78IE0pefD8l+GJi2Dv2uMf\nRESkGyk4+oDYqEh+evkEfnzZeJ7akcGsiu9Rct7PoXQTPPJJeOVOdV+JSI9RcPQRZsbnzxjK/33p\ndPbXNHH+G0N5a/arcNpN8P4jXvfVqvnQ3BzqqopIP6fg6GPOGJ7Joq+eRX5GAl9YsJmHk27H3boY\n0obAC1+B354F615QgIhI0Cg4+qC89ASeu+1MLpkwmP/+20buWAo1X/gbXPE4NDfCn25UgIhI0Cg4\n+qj4mEh+fd0U7pw9mr+u2c2Vj7xHUd7F8O/vwWd/B00NfoCcDetfVICISLdRcPRhZsZtM0cw78bp\n7Cyv5jMP/pNX1pXgJlwJt7/vB0g9LPyCAkREuo2Cox84b/QAXrj9LAanxnHb0x9w61Mr2HWoHiZe\n5QfIY9BU5wXII+fA+kUKEBE5YVpypB9pbGrmiX9u5/7XPsIMvjFrFDedWUBkhEFzE6x9Dt78byjb\nAgMnwDn/AaM/DVExoa66iPQCnV1yRMHRDxWWV/O9F9eyZFMpE/NS+enlExifm+ptbGo8HCDlH0N8\nOoy/EiZfBzlTwSy0lReRkFFwhHFwADjn+OuaPfzwL+vZX13PF88q4P/71KkkxER5OzQ3wceLYfUz\nsPElaKyFrFEw6VqYeA2ktnujRRHpxxQcYR4cLQ5WN3Df3zYyf9lOctPi+cnl4zlv1IC2O9Ue9Kbu\nrp4PO98FDIafC5M+B2M+DTGJIam7iPQsBYeCo43l28u5+88fsqWkkk9PHMz3Lx3LgOS4o3cs3+ot\noLh6PhzYCTFJMPYyryUy9CyI0HwKkf5KwaHgOEpdYxOPvLmVB9/YQlx0BN+ePZprp+cTFdlOGDQ3\ne62P1c/AuhehvgLSC2DqF2DyDZA8sMfrLyLBpeBQcHTo49JKvvv8h7y3tZzh2Yl8a9YoZo8fhHU0\nMF5fDRv/Ch88BdvfgogoGHWRt07W8H9TK0Skn1BwKDiOyTnH39cX8/NXN7GlpJJJeancOXs0Z56S\ndewP7tsCHzwJq56G6jJIHeK1QqZcDyk5PVN5EQkKBYeCo1Oamh3PfVDEA699xO6DtZwzMotvXzia\nCXmpx/5gYz1seglW/h62LgGLhFMv9Fohp1wAEZE9UHsR6U4KDgVHl9Q2NPF/7+3gocVb2F/dwCUT\nB/ONT53K8Oyk43+4fKvXjfWvp6GqBFLyYMoNXnfWoAkKEZE+IuTBYWbzgE8DJc658X7ZVcAPgDHA\nDOfcioD97wZuAZqAO5xzr/rls4FfApHA75xz9x3vuxUcJ+5QbQO/W7qV3729jbrGZq6Zns/Xzh/J\nwJR2ZmAdqakBNr3itUI+fgNwEJsKQz8BBWd7j0ETFSQivVRvCI5PApXAUwHBMQZoBh4BvtkSHGY2\nFpgPzABygNeBU/1DfQR8CigClgPXOefWH+u7FRwnr7Sijgff2Mwzy3YSGWHcdOYwvnLucNISOrk8\nScVe2P62N5i+/W1vmROA2BQYckSQREYF70REpNM6GxxB+xvrnFtqZgVHlG0A2pu9MwdY4JyrA7aZ\n2Ra8EAHY4pzb6n9ugb/vMYNDTl52ciw/nDOeW84ezv2vbeKRpR/zh3e3c92MIdxyzjAGp8Yf+wDJ\ng2DCld4DAoLEf2x+1StvCZKhZ0LuaTB4EsSlBPXcROTk9Jb/6uUC7wW8L/LLAAqPKD+9vQOY2Vxg\nLsCQIUOCUMXwNCQzgQeuncJAn70nAAAUQklEQVRXZo7g4SUf88Q723ny3e3MmZzLV84dzikDkjt3\noI6CZMc/2wYJBlkjvXWzcqdCzhRvnCT6OEElIj2mtwTHSXPOPQo8Cl5XVYir0++MHpTCL6+dwjdn\njeJ3b23ljysKeXZlEReMGchtM4dz2tCMrh3wyCCpKoPd/4LdH8CuD2DrYlizwNsWEQUDxngh0hIo\nA8ZCZHT3nqSIdEpvCY5dQH7A+zy/jGOUSwjkZyTwwznjueP8kTz57g6eenc7VzxczPSCdL5y7gjO\nGzWAiIgTWGE3MRNGXuA9AJyDij1eiLSEyfpF3uwtgMgYGDjO69pqeQwYB9GdGMQXkZMS1Om4/hjH\nX1sGxwPKl9B2cHwc8AyHB8f/AYwEDG9w/Hy8wFgOfM45t+5Y36vB8Z5TXd/IgmWFPP72NnYdqOHU\ngUl8+ZMj+MzkHKLbW8rkZDgH+7d5IbJnFexZA3tWQ+0Bb7tFei2TQRMPh8mgCRDbiSnFItIrZlXN\nB2YCWUAxcA9QDvwayAYOAKuccxf6+38X+CLQCHzdOfeKX34x8ADedNx5zrl7j/fdCo6e19DUzF9W\n7+aRN7eyqbiCnNQ4rpsxhCtOyyMnLYjjE855izHuWd32UVXi72CQeQrkTYP8071H9mgtkyLSjpAH\nRygpOELHOceSTaU89tZW3vm4DDM4+5QsrpqWz6yxA4mL7qFrOCr2Hg6R3augaBlUlXrbYlMhfzrk\nnwH5M7xQ0dLxIgoOBUfoFZZX86eVRTy3sohdB2pIiYvisim5XHVaPuNzUzpeVDEYnPOucC9cBoXv\nec8lGwDndXENGt82SFJyNfguYUfBoeDoNZqbHe98XMafVhbyytq91Dc2M3pQMldNy+eyyTlkJsWG\npmI1B6BoORS+Dzvfg10roaH68PaETEgaBEkDIGng4efkI8ri0nTLXekXFBwKjl7pYHUDi9bs5tkV\nhawuOkh0pHH+6IFcNS2PT56a3f0D6l3R1AjFH3pdW5XF/qPE6/aqLPHeN9Ud/bnIWO/+JEmDvOfk\nwQEBM8h7Th4E8RkaW5FeTcGh4Oj1Nu2t4E8rCnn+X7soq6onLSGai8YP4tJJOZw+LJPIE5nWG0zO\nebfZbQmRlkfF3sPPFXuhcq+335Eior1AScz0WjPxGZCQ0fZ1fLr3PiHDK4tJVGtGeoyCQ8HRZ9Q3\nNrP0o1L+smY3r60vprq+iezkWC6ZMJhLJ+UwdUhaz46HdIeGmoBA2QMVxV6gVOyFqn1QUw7V5d5z\neyHTIioO0oZAxvCAxzDvOXWI1vmSbqXgUHD0STX1TbyxsYS/rN7NG5tKqG9sJjctnksn5XDppMGM\nHdzDg+o9oakRavYfDpPqssOvq0rhwA4o3+YN7geOwUREeaGSPuxwqKTkgEV4rRSLAMxvsXTwHBUH\nscneIy7Ve9akgLCl4FBw9HkVtQ28tr6YRat38/bmfTQ2O4ZnJ3LpxBwunjCYUwcm9b8QORbnvBZM\n+daAx7bDr+sOdc/3RMUHhEmK/zrFe7R0rSVmQUJWwOtML3hO9PdobgJMY0AhpuBQcPQr5VX1/G3t\nXv6yejfvbSvDOchJjePcUQM4b1Q2Z52SRWJsGHfbOOe1UCr2AM5775oPv8aB44j3zdBYC3UV3qP2\nkP/6YDtlh7zXNeVtWz2BIqIDgiTDC5KmRu87GuuO/dzc4LWgWiYVJA8+PKkgOadtWXx69437OOd9\nf0ONd14NNVBf5f3ZxKd75xGb0jPjTE2N3p9vVanXnVlV6rU+aw/6LchI7142bZ4jji5PzPLuwnkC\nFBwKjn6r5FAtb2wsYfGmEv65pYzKukaiI43pBRmcN2oAM0dlc8qAMGuN9KT6aqje5/2jVlXmva7a\nd3RZ7UFvTbGoOIiKPeK5nbLG2oAxob3ec83+o7+/ZRZbdOLhLrnW55buuYi2D4DGGj8Yqg+HREM1\nfqJ2zCK9EGkJkvh0b+JCfDok+OWRMV6ryTV7j+YmcE1HPPvlzX7XZEswVJV6j/bO9UTkToNb/3FC\nH1VwKDjCQn1jMyt37GfJphKWbCplU3EFALlp8cwclc3MUQM4c0RmeLdG+rKGGi9MDu1pGygVe72g\ncc1HtK6a23n4razoeP+RADEJ3nPL+5bXMYn+Ev52eNypZr8/kSHw/X7vuaGq6+dkEV7YJGZ73X2J\nLY9sv8WWfbgrMDHba7nhvMBpL4iODKjIGEjLP2412q2agkPBEY52H6hhyaZSlmwq4Z9b9lFV30RM\nZAST8lOZXpDB9GEZnDY0nZQ4DQBLN2io9QKkuaGdrqSIdrqWInv19GoFh4Ij7NU3NrNiezlvflTK\ne9vKWbfrII3Njgjz7i8yY1iGHybpDEjWcuwiCg4Fhxyhur6RVTsPsGx7Ocu3l/PBjgPUNDQBUJCZ\nwLSCDGb4rZKCzASNkUjYCfk9x0V6m4SYKM48JYszT8kCvKXg1+0+xPJt5SzbXs4/NhTz7MoiwLvn\n+oyCDKYXpDN9WAajB6X0vivZRUJELQ4RX3Oz4+PSSq9Fsq2cZdvK2X2wFoDkuCimDfVC5PRhGUzI\nTSMmStccSP+iFodIF0VEGCMHJjNyYDLXnz4UgKL91SzfXs6ybftZtq2MxZu8e3rERkUwOT+N04dl\nMK0gg0n5aaTGa8BdwoOCQ+QY8tITyEtP4PIpeQDsq6xjhR8ky7eX8+DiLTT7jfYR2YlMzk9nypA0\nJuenMXpQMlGhXO1XJEjUVSVyEipqG1hdeJBVhfv5184DrCo8QFlVPQDx0ZFMyE1tDZIpQ9IZlKrZ\nW9J7qatKpAckx0Vz9sgszh7pDbg75ygsr+FfhftZVXiAf+08wBP/3E59UzMAg1LiGJeTwsiByZw6\nMImRA5I5ZUAS8TE9dEtdkW6g4BDpRmbGkMwEhmQmMGdyLgB1jU2s332IVYVei2TjngqWbi6locn5\nn4H89AQvSI4IlB67R7tIFwQtOMxsHvBpoMQ5N94vywD+CBQA24GrnXP7zZsw/0vgYqAauMk594H/\nmRuB//QP+xPn3JPBqrNIMMRGRTJlSDpThqS3ljU0NbOjrIqPiiv5qLiCzSWVbC6uYMmmUhqbDwfK\nkIwERg5IZuTAJEYO8AJlxIBEEmL0fz4JnaCNcZjZJ4FK4KmA4PgZUO6cu8/M7gLSnXN3mtnFwP/D\nC47TgV865073g2YFMA1vJbKVwGnOuWOuBqYxDumrGpqa2b4vMFAq2FJSybZ9Va0tFIC89HgvSAZ6\nLZOWh5ZSkZMR8jEO59xSMys4ongOMNN//SSwBLjTL3/KeSn2npmlmdlgf9/XnHPlAGb2GjAbmB+s\neouEUnRkROuU4EsY3FrutVCq2VJSwebiSjaXVLKlpJJ3Pi6jrrG5db9BKXGMHJjEqX6X1ykDvOdk\nBYp0o55u7w50zu3xX+8FBvqvc4HCgP2K/LKOyo9iZnOBuQBDhgzpxiqLhF50ZERrq2L2+MPlTc2O\nov3VrWGy2e/2evr9HdQ2HA6Uwalx3vjJAC9URg70jqVAkRMRso5S55wzs27rJ3POPQo8Cl5XVXcd\nV6Q3i4wwhmYmMjQzkQvGDmwtb252FO2v4aPiCj7yWykfFVfwh61tWygDkmPJz0hgSEYC+enx5GUk\nkJ/uDe4PSonTMivSrp4OjmIzG+yc2+N3RZX45buAwAXk8/yyXRzu2mopX9ID9RTp0yIiDs/uCgyU\npmZHYXl164D89n1VFO6vZtm2cl5cVdN6MSNAdKSRkxbPkAzvIsj8DO/10IxEhmQkkJqg1kq46ung\nWATcCNznP78YUP5VM1uANzh+0A+XV4GfmlnLdJRZwN09XGeRfiMywijISqQgK5FZ49puq29sZs/B\nGgrLayjcX83O8moKy6sp3F/D39ftbb2wsUVKXBRDM70Qyc9IYGim13IZkpHA4NQ4XTXfjwVzOu58\nvNZClpkVAffgBcZCM7sF2AFc7e/+Mt6Mqi1403FvBnDOlZvZj4Hl/n4/ahkoF5HuFRMV0drt1Z6q\nukYK91ezo8wLlB1lXris33OIv6/f22bWV1SEkZsez7CsRIZlJTLcD6thWYnkpMYToS6wPk1LjojI\nSWtqduw5WMPO8mp2+oGyo6yabfuq2LavqvW+J+AtEFmQ6YXIsOxEhmV6zwWZiWQlxeg+KCEU8um4\nIhI+IiOsdUHIM0e03eaco6Sijq2lVX6QVLJtXzWbSyr4x8biNi2VpNgohmYmUJCZ6D1neYFSkJlA\ndnKsQqWXUHCISFCZGQNT4hiYEscnRmS22dbY1MyuAzVs3VfFjn1VbC+rZntZFev3HOLVdXtbr6IH\nSIiJZKgfIkMzE8lLjycnLY7BqfHkpMWTEhelYOkhCg4RCZmoyIBxlVFttzU0NbP7QA3by6rZUea1\nVnaUVbOpuILXN7RtqQAkxkQyOC2ewalx5KbFMzg1nsFpceSkegGTkxavtb+6iYJDRHql6MBQIbvN\ntqZmR2lFHbsP1rD7QA17DtSy++Dh5w17KthXWXfUMQemxJKf7s0CC7xuJT/DCxpdt9I5Cg4R6XMi\nI4xBqXEMSo1jasDikYHqGpsoPuiFy679NRTt96YZF5a3f91KVIR33Up+Rjz56QnkpceTn+E956Un\nkJ0Uq9lgPgWHiPRLsVGRrRdBtufI61ZarlkpLK/m9Q3F7Ktse91KTFQEeWnx5AYESkvA5KUnhNWM\nMAWHiISl4123UlPfxK4DXpgUlVdTFNBqWbd2L+VHXBAZFx3hXWHvB0l+hv/sh0taQnS/CRYFh4hI\nO+JjIjllQDKnDEhud3tlXSO7/BZK0f7q1lAp2l/Dyh37OVTb2Gb/pNio1tbJiOxERmQnMWKA95yW\nENMTp9RtFBwiIicgKTaKUYOSGTWo/WA5WNNwOFBaWyzV7CyvYunmUuoDFpvMTIxhxIAkL0yyExkx\nIIlTspPITeudV9krOEREgiA1PprU+FTG5aQeta2p2bFrfw1bSiv4uKSKj0sr+bi0klfXte0Ci42K\nYKh/8WNmYiyZSTFkJcWSkRhDZmIMmUmxZCV5z4kxkT3WFabgEBHpYZEBqxf/2+i228qr6r0gKfHC\nZEdZNWVV9awpOkBZZT0VdY3tHjMmKoKsxBhOK8jg19dNCWr9FRwiIr1IRmIMGYkZTC/IaHd7XWMT\n5VX1lFXWs6+yjrLKesqq6ijzywYkxwa9jgoOEZE+JDYq0rsqPjU+ZHXQgvkiItIlCg4REekSBYeI\niHSJgkNERLpEwSEiIl2i4BARkS5RcIiISJcoOEREpEvMOXf8vfoYMysFdpzEIbKAfd1Und4sXM4T\nwudcw+U8IXzOtSfPc6hzLvt4O/XL4DhZZrbCOTct1PUItnA5Twifcw2X84TwOdfeeJ7qqhIRkS5R\ncIiISJcoONr3aKgr0EPC5TwhfM41XM4Twudce915aoxDRES6RC0OERHpEgWHiIh0iYIjgJnNNrNN\nZrbFzO4KdX2Cycy2m9mHZrbKzFaEuj7dyczmmVmJma0NKMsws9fMbLP/nB7KOnaHDs7zB2a2y/9d\nV5nZxaGsY3cws3wzW2xm681snZl9zS/vj79pR+faq35XjXH4zCwS+Aj4FFAELAeuc86tD2nFgsTM\ntgPTnHP97gIqM/skUAk85Zwb75f9DCh3zt3n/6cg3Tl3ZyjrebI6OM8fAJXOuV+Esm7dycwGA4Od\ncx+YWTKwErgMuIn+95t2dK5X04t+V7U4DpsBbHHObXXO1QMLgDkhrpOcAOfcUqD8iOI5wJP+6yfx\n/jL2aR2cZ7/jnNvjnPvAf10BbABy6Z+/aUfn2qsoOA7LBQoD3hfRC3+wbuSAv5vZSjObG+rK9ICB\nzrk9/uu9wMBQVibIvmpma/yurD7ffRPIzAqAKcD79PPf9IhzhV70uyo4wtfZzrmpwEXA7X63R1hw\nXv9sf+2jfRgYAUwG9gD/E9rqdB8zSwKeA77unDsUuK2//abtnGuv+l0VHIftAvID3uf5Zf2Sc26X\n/1wCPI/XVdefFfv9xy39yCUhrk9QOOeKnXNNzrlm4DH6ye9qZtF4/5A+7Zz7s1/cL3/T9s61t/2u\nCo7DlgMjzWyYmcUA1wKLQlynoDCzRH/gDTNLBGYBa4/9qT5vEXCj//pG4MUQ1iVoWv4h9V1OP/hd\nzcyAx4ENzrn7Azb1u9+0o3Ptbb+rZlUF8Ke4PQBEAvOcc/eGuEpBYWbD8VoZAFHAM/3pXM1sPjAT\nbznqYuAe4AVgITAEb8n9q51zfXpguYPznInXneGA7cCXA8YB+iQzOxt4C/gQaPaLv4PX99/fftOO\nzvU6etHvquAQEZEuUVeViIh0iYJDRES6RMEhIiJdouAQEZEuUXCIiEiXKDhEehkzm2lmfw11PUQ6\nouAQEZEuUXCInCAzu8HMlvn3R3jEzCLNrNLM/te/l8I/zCzb33eymb3nL1L3fMsidWZ2ipm9bmar\nzewDMxvhHz7JzJ41s41m9rR/RbFIr6DgEDkBZjYGuAY4yzk3GWgCrgcSgRXOuXHAm3hXcwM8Bdzp\nnJuId1VwS/nTwEPOuUnAmXgL2IG3KurXgbHAcOCsoJ+USCdFhboCIn3U+cBpwHK/MRCPt8heM/BH\nf5//A/5sZqlAmnPuTb/8SeBP/nphuc655wGcc7UA/vGWOeeK/PergALg7eCflsjxKThETowBTzrn\n7m5TaPa9I/Y70TV96gJeN6G/q9KLqKtK5MT8A7jSzAZA6/2vh+L9nbrS3+dzwNvOuYPAfjM7xy//\nPPCmf4e3IjO7zD9GrJkl9OhZiJwA/S9G5AQ459ab2X/i3UUxAmgAbgeqgBn+thK8cRDwlv3+rR8M\nW4Gb/fLPA4+Y2Y/8Y1zVg6chckK0Oq5INzKzSudcUqjrIRJM6qoSEZEuUYtDRES6RC0OERHpEgWH\niIh0iYJDRES6RMEhIiJdouAQEZEu+f8BOqIWbc9A1TcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:  1124.5845788795123\n",
      "Test MAE:  13.370739080964048 \n",
      "\n",
      "Results for model:  3 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:  1141.255510131439\n",
      "Test MAE:  13.579944546795234 \n",
      "\n",
      "SAMPLE DATA\n",
      "Actual\t Model 1 Prediction\t Model 2 Prediction\t Model 3 Prediction\t\n",
      "98.0 \t 84.26058 \t\t 86.39358 \t\t 83.37668 \n",
      "\n",
      "24.0 \t 28.362635 \t\t 34.47313 \t\t 35.86448 \n",
      "\n",
      "68.0 \t 77.836815 \t\t 63.145313 \t\t 76.45101 \n",
      "\n",
      "38.0 \t 38.84139 \t\t 35.377316 \t\t 35.849197 \n",
      "\n",
      "26.0 \t 20.591122 \t\t 31.600594 \t\t 25.05265 \n",
      "\n",
      "22.0 \t 22.694153 \t\t 26.281837 \t\t 25.950684 \n",
      "\n",
      "15.0 \t 22.50017 \t\t 21.959805 \t\t 22.527153 \n",
      "\n",
      "12.0 \t 18.67806 \t\t 16.7112 \t\t 11.182672 \n",
      "\n",
      "25.0 \t 22.023197 \t\t 20.858475 \t\t 18.970558 \n",
      "\n",
      "40.0 \t 31.238073 \t\t 30.464008 \t\t 30.31108 \n",
      "\n",
      "25.0 \t 20.429182 \t\t 17.428871 \t\t 17.000938 \n",
      "\n",
      "20.0 \t 17.56041 \t\t 19.404667 \t\t 16.847946 \n",
      "\n",
      "45.0 \t 30.212835 \t\t 32.540287 \t\t 28.607475 \n",
      "\n",
      "20.0 \t 23.079685 \t\t 17.604946 \t\t 18.688309 \n",
      "\n",
      "30.0 \t 35.713364 \t\t 41.159313 \t\t 40.657215 \n",
      "\n",
      "13.0 \t 17.704117 \t\t 13.871768 \t\t 14.491464 \n",
      "\n",
      "15.0 \t 31.783463 \t\t 24.641483 \t\t 32.955513 \n",
      "\n",
      "20.0 \t 31.086563 \t\t 27.162975 \t\t 24.123642 \n",
      "\n",
      "53.0 \t 44.038662 \t\t 48.39953 \t\t 53.347305 \n",
      "\n",
      "14.0 \t 29.06331 \t\t 38.051247 \t\t 31.014957 \n",
      "\n",
      "20.0 \t 19.353354 \t\t 20.719028 \t\t 17.331566 \n",
      "\n",
      "20.0 \t 13.384002 \t\t 5.2662535 \t\t 15.091935 \n",
      "\n",
      "45.0 \t 68.478424 \t\t 56.74687 \t\t 53.746616 \n",
      "\n",
      "21.0 \t 18.32455 \t\t 23.804962 \t\t 18.293236 \n",
      "\n",
      "160.0 \t 85.953285 \t\t 60.095566 \t\t 70.61398 \n",
      "\n",
      "34.0 \t 33.246685 \t\t 34.11015 \t\t 30.599533 \n",
      "\n",
      "45.0 \t 63.286655 \t\t 62.94825 \t\t 72.70795 \n",
      "\n",
      "13.0 \t 12.593459 \t\t 8.010875 \t\t 5.7476125 \n",
      "\n",
      "69.0 \t 86.87836 \t\t 83.197975 \t\t 88.82585 \n",
      "\n",
      "75.0 \t 63.870018 \t\t 76.20642 \t\t 76.34749 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(3):\n",
    "  print(\"Results for model: \", i+1,\"\\n\")\n",
    "  plot_losses(model_history[i])\n",
    "  print(\"Test MSE: \", mean_squared_error(labels_test, model_predictions[i]))\n",
    "  print(\"Test MAE: \", mean_absolute_error(labels_test, model_predictions[i]),'\\n')\n",
    "\n",
    "# Print some example predictions\n",
    "num_predict = 30\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"SAMPLE DATA\")\n",
    "print('Actual\\t', 'Model 1 Prediction\\t', 'Model 2 Prediction\\t','Model 3 Prediction\\t')\n",
    "for i in np.random.randint(len(test_seqs_padded), size = num_predict):\n",
    "    print(labels_test.iloc[i],'\\t', model_predictions[0][i][0],'\\t\\t', model_predictions[1][i][0],'\\t\\t', model_predictions[2][i][0],'\\n')\n",
    "      \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wine_price_pred.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
